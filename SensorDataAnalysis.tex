% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{wrapfig}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Industry 4.0: Sensor Data Analysis using Machine Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Nadeem Iftikhar\inst{1} \and
Finn Ebertsen Nordbjerg\inst{1}\and
Thorkil Baattrup-Andersen\inst{2}\and
 Karsten Jeppesen\inst{1}}
%
\authorrunning{N. Iftikhar et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University College of Northern Denmark, Aalborg 9200, Denmark 
\email{\{naif,fen,kaje\}@ucn.dk}\\
 \and
Dolle A/S, Fr\o strup 7741, Denmark\\
\email{ta@dolle.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The technological revolution, known as industry 4.0, aims to improve efficiency/productivity and reduce production costs. In the Industry 4.0 based smart manufacturing environment, machine learning techniques are deployed to identify patterns in live data by creating models using historical data. These models will then predict previously undetectable incidents. This paper initially performs a descriptive statistics and visualization, subsequently issues like classification of data with imbalanced class distribution are addressed. Then several binary classification-based machine learning models are built and trained for predicting production line disruptions, although only logistic regression and artificial neural networks are discussed in detail. Finally, it evaluates the effectiveness of the machine learning models as well as the overall utilization of the manufacturing operation in terms of availability, performance and quality.



\keywords{Industry 4.0  \and Sensor Data \and Data Analysis \and Machine Learning \and Smart Manufacturing} \and {Imbalanced Data}
\end{abstract}
%
%
%
\section{Introduction}
The fourth industrial revolution (Industry 4.0) focuses greatly on automation, interconnected devices and sensors, machine learning, data analysis and visualization. Industry 4.0 aims at enhancing productivity by increasing operational efficiency, development of new products, services and business models. \cite{industry}. 
Data analysis uses many techniques ranging from statistics to machine learning. In the manufacturing industry, these techniques can be applied to a number  of problems including, but not limited to: identify or predict production line interruptions; identify bottlenecks, and optimize the manufacturing processes in order to minimize downtime and maximize productivity. The framework presented in this paper to structure and execute the data analysis and modeling methods is Cross-industry Standard Process for Data Mining (CRISP—DM) \cite{CRM}. 
It enlists business objectives, data insight, data preprocessing, modeling and evaluation. This work was achieved in collaboration with Dolle \cite{Dolle}. 
Dolle is a leader in manufacturing of timber loft ladders and is present in more than 40 countries worldwide. Competing globally require an efficient production process and a rigorous quality control in order to maintain profitability and productivity. This has led the development of these sensor data analysis and machine learning techniques. A main goal of building machine learning methods for Dolle is to reduce downtime and improve profitability by predicting production line disruptions.

This paper is a significant extension of our previous conference paper \cite{nadeem}. In the previous work a data pipeline to handle data acquisition, processing and analysis was proposed. An exploratory analysis of the data was provided. Further, a statistically based machine learning model to predict costly production line disruptions was also presented using a real-life case study. This paper extends \cite{nadeem} by building multiple machine learning models and evaluating the performance of these models to select the best one(s) and it also presents techniques to handle imbalanced data.

To summarize, the main contributions in this paper are as follow:
\begin{itemize} 
\item Providing an in-depth descriptive statistics and visualization. 

\item Presenting techniques for handling imbalanced data.

%\item It investigated how to uncover hidden patterns in the data.
\item Building multiple machine learning models for predicting costly production line disruptions.

%\item It implemented a near real-time dashboard that displays the input/output pace.
\item Comprehensive evaluation of the equipment effectiveness and the performance of the proposed models.
\end{itemize} 

The paper is structured as follows. Section~\ref{sec:businessunderstanding} describes the objectives and requirements from a business perspective. Section~\ref{sec:dataunderstanding} gives initial insights about data. Section~\ref{sec:datapreparation} provides descriptive statistics. Section~\ref{sec:modeling} presents
the machine learning models. Section~\ref{sec:evaluation} evaluates the equipment effectiveness and performance of the models. Section~\ref{sec:relatedwork} presents the related work. Section~\ref{sec:conclusionandfuturework} concludes the paper and points out the future research directions.

\section{Objectives}
\label{sec:businessunderstanding}
The focus of this section is to understand the basic concepts of smart manufacturing in consultation with domain experts. The objectives of the project are derived from the viewpoint and requirements of Dolle. Properly scrutinized they are then translated to data science problems. From a business perspective Dolle's primary questions include: How long does it take, before the right output pace is achieved after a machine is started (see Fig.~\ref{fig:ladder1})? Relating to the output pace being defined as the average time between the start of manufacturing of one unit and the start of manufacturing of the next unit: What is the current rate? What is the optimal rate? What are the causes and length of production stops? In addition, Dolle would like to know the amount of time spent changing from one product type to another.  

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{machine1}
\caption{Three-section timber loft ladder at Dolle's assembly line}
\label{fig:ladder1}
\end{figure}

After defining the goals in business terminology, these goals are then translated to technical terms, known as data mining goals. A non-exhaustive list of the data mining goals is presented below:
\begin{enumerate} 
\item Frequency of machine stops due to faulty PAR/screw errors as well as total downtime of the machine due to faulty PAR/screw errors?
\item What is the production rate (pace) of the machine?
\item What is the maximum pace?
\item When do delays occur and for how long and are there any discernable patterns?
\item Based on historical patterns, predict machine stops and/or how to prevent them?
\item What is the overall downtime of a machine and/or what are the costs?
\end{enumerate} 


In general, production with 80-85 \% efficiency is considered very efficient. It will be interesting to investigate every predicted and unpredicted incident during production: What caused it? Can it be predicted(if unpredicted) and mitigated? Some of the known challenges during production are: breakdowns, changeovers, minor stoppage, reduced speed, defects and setup scrap. Hence, the quality of the manufacturing process can be measured by calculating the \textit{Overall Equipment Effectiveness (OEE)} \cite{oee}. OEE is the most common standard for measuring manufacturing productivity. It calculates the percentage of manufacturing time that is truly productive. 

\section{Initial Insights about Data}
\label{sec:dataunderstanding}
This section starts with the data acquisition and proceeds with the activities which describe how initial insights of the data are obtained. Firstly, the data acquisition is described and hereafter, activities that provide understanding of the data are discussed. These activities include getting the first insight into the data, identifying data for analysis purposes, discovering data quality issues and/or detecting interesting subsets to form hypothesis regarding previously undetected patterns. The data provided by Dolle is machine data and ERP system data (Fig.~\ref{fig:data}). Machine data describe the state of sensors and alarms, whereas, ERP data provides information about products, job executions and work calendar. The machine data consists of only binary values (0's and 1's). The number of attributes depends on the specific machine in question. The product data set contains 85 attributes, the job execution data set contains 69 attributes and the work calendar data set has 10 attributes. Each job represents a specific business task that is carried out for a certain time interval to produce particular type of ladders. The structure of the data does not conform to any standard and additionally no assumptions can be made that two identical units or machines display identical structures. 

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data} 
 \vspace{-7mm}
\caption{Data overview}
\label{fig:data}
\end{figure}

\begin{table*}[ht]
\caption{Sensor and alarm data \cite{nadeem}}
\label{example}
\centering
\begin{tabular}{c|ccccccc}
\hline\noalign{\smallskip}
\emph{Id} & DateTime & MachineOn & PaceIn & PaceOut &  FaultyString  & ScrewError & Alarm\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
\emph{.} & . & . & . & . & . & . & . \\
\textbf{\emph{1}} & \textbf{19-02-2019 09:53:07}  & \textbf{1} & \textbf{0} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\emph{2} & 19-02-2019 09:53:09  & 1 & 1 & 1 &  0 & 0 & 0\\
\emph{3} & 19-02-2019 09:53:10  & 1 & 0 & 1 &  0 & 0 & 0\\
\emph{4} & 19-02-2019 09:53:12  & 1 & 0 & 0 &  0 & 0 & 0\\
\emph{.} & . & . & . & . & . & . & . \\
\emph{5} & 19-02-2019 09:53:56  & 1 & 1 & 0 &  0 & 0 & 0\\
\emph{6} & 19-02-2019 09:53:58  & 1 & 0 & 0 &  0 & 0 & 0\\
\textbf{\emph{7}} & \textbf{19-02-2019 09:54:04}  & \textbf{1} & \textbf{0} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\emph{8} & 19-02-2019 09:54:09  & 1 & 0 & 0 &  0 & 0 & 0\\
\emph{9} & 19-02-2019 09:54:14  & 1 & 1 & 0 &  0 & 0 & 0\\
\emph{10} & 19-02-2019 09:54:15  & 1 & 0 & 0 &  0 & 0 & 0\\
\textbf{\emph{11}} & \textbf{19-02-2019 09:54:20}  & \textbf{1} & \textbf{0} & \textbf{0} &  \textbf{0} & \textbf{0} & \textbf{1}\\
\emph{12}& 19-02-2019 09:54:21  & 1 & 1 & 0 &  0 & 0 & 1\\
\emph{.} & . & . & . & . & . & . & . \\
\textbf{\emph{13}} & \textbf{19-02-2019 09:56:14}  & \textbf{1} & \textbf{0} & \textbf{0} &  \textbf{0} & \textbf{1} & \textbf{0}\\
\emph{14} & 19-02-2019 09:56:16  & 1 & 0 & 0 &  0 & 0 & 0\\
\textbf{\emph{15}} & \textbf{19-02-2019 09:56:29}  & \textbf{1} & \textbf{0} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\textbf{\emph{16}} & \textbf{19-02-2019 09:56:31}  & \textbf{1} & \textbf{1} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\textbf{\emph{17}} & \textbf{19-02-2019 09:56:33}  & \textbf{1} & \textbf{0} & \textbf{0} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\emph{.} & . & . & . & . &  . & . & . \\
\hline
\end{tabular}
\end{table*}

Dolle's case study clearly illustrate the challenges faced in data analysis in the smart manufacturing industry. The data analysis methods presented in this paper, however, are general. In this case study machine data from the production facility are logged in order to record the states of the machines at any given time. The logged data is initially kept in detailed format in different database tables (a separate table for each machine). As mentioned above, each machine has a different set of sensors/attributes, for that reason only one of the machines is considered for demonstration purposes. The selected machine (\emph{Machine\_1}) consists of the following attributes: \emph{(DateTime, MachineOn, PaceIn, PaceOut, FaultyString, ScrewError, Alarm)}. The DateTime identifies a recording of a date and time event at a second granularity. The \emph{MachineOn} sensor indicates the machine is running for a given job. The \emph{PaceIn} or entrance of a beam/PAR sensor represents an incoming PAR. The \emph{PaceOut} or exit of a ladder sensor represents an outgoing ladder. The \emph{FaultyString} sensor signifies the quality of the PAR. Bended or twisted PARs are regarded as faulty PARs. The \emph{ScrewError} sensor corresponds to the screwing machine that screw PARs into place. Finally, the \emph{Alarm} or Error sensor represents a general abnormality in the machine. For example, a PAR is struck in the machine. An alarm for extended time may result in a  machine stoppage.

In order to provide a snapshot of data, a real machine data set provided by Dolle is used. The snapshot contains 7 attributes for job no. 307810 to produce a CF (ClickFix) type ladder. In Table~\ref{example}, initially the granularity of the detailed data is at \emph{second by job by machine}. Ex: Row number 1 reads as follows: \emph{DateTime}=19-02-2019 09:53:07 (represents: second granularity. It is important to note that if the next row has same values as the previous row in that case the next row will not be logged to the database causing the time discontinuity), \emph{MachineOn}=1 (represents: machine is running), \emph{PaceIn}=0 (represents: no PAR is entering), \emph{PaceOut}=1 (represents: exiting of the ladder), \emph{FaultyString}=0 (represents: the quality of the PAR is OK), \emph{ScrewError}=0 (represents: no error in the screwing machine) and \emph{Alarm}=0 (represents: no abnormality in the machine). Whereas, Id is an abstract attribute and used only for row identification purposes. Similarly, row number 13 reads as follows: \emph{DateTime}=19-02-2019 09:56:14, \emph{MachineOn}=1, \emph{PaceIn}=0, \emph{PaceOut}=0 (represents: no ladder is exiting), \emph{FaultyString}=0, \emph{ScrewError}=1 (represents: an error in the screwing machine) and \emph{Alarm}=0 (represents: no abnormality in the machine identified yet). Further, initial look into the data in Table~\ref{example} reveals some interesting facts, such as, the ladder is produced (row 7) in 09:54:04 - 09:53:07 = 57 seconds (average pace), where as, the next ladder is produced (row 15) in 09:56:29 - 09:54:04 = 145 seconds. The delay in the production of the next ladder is due to the fact that an alarm has been triggered (row 11) and a screwing machine error has also caused the delay (row 13). 


\begin{table}[ht]
\caption{ERP system data}
\label{example2}
\centering
\begin{tabular}{ccccccc}
\hline\noalign{\smallskip}
JobId & ProductId & JobStart & JobEnd &  JobFinished & GoodQuality\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
. & . & . & . &  .   \\
307810  & 524167 & 19-02-2019 09:34:23  & 19-02-2019 10:17:43   & 0 & 0\\
370810  & 524167 & 19-02-2019 10:28:28  & 19-02-2019 11:23:04   & 1 & 70\\
. & . & . & . &  .   \\
\hline
\end{tabular}
\end{table}

Further, Table~\ref{example2} displays some of the attributes of the ERP system data that may be used for analysis purposes \emph{(JobId, ProductId, JobStart (date and time), JobEnd (date and time), JobFinished, GoodQuality)}. As mentioned earlier, ERP system data has 164 attributes, in total. The \emph{JobId} represents the given job. The \emph{productId} represents a specific product. The \emph{JobStart} represents the date and time when a job starts. Similarly, \emph{JobEnd} represents the date and time when a job ends. A job may be carried out in multiple time intervals as seen in Table~\ref{example2}. The \emph{JobFinished} characterizes if the current job has finished and the \emph{GoodQuality} attribute represents the number of acceptable quality products produced. 

Furthermore, several interesting subsets may be identified from the initial insights of the data sets that leads to hypothesis regarding initial data patterns. For example, whether screwing machine errors causes more machine stops than faulty PARs. 



\section {Data Preprocessing}
\label{sec:datapreparation}
This section provides insight into the business problems before performing data modeling. The data preprocessing phase include activities, such as data selection, data transformation, data cleaning and data validation. These tasks may be performed several times and not in any given order. During this phase important issues like selecting the relevant data, cleaning of data and discarding unacceptable data are addressed. Additionally, it is determined how the ERP system data can be integrated into the final data sets. Some of the cleaning techniques discussed in \cite{iftikhar} may be applicable here as well. Metadata originating from discussions between data scientists and domain experts has been crucial to the process of data validation. Some meta issues cannot be inferred from the sensor data but require domain expertise like: ``is the machine output reliable, when the `error sensor or alarm' is `on', can this be verified?". Contrary to logic, the answer is ``yes", as during production of certain types of ladders the alarm is disregarded. Another anomaly is that the logged data showed twice the numbers of ladders produced than actually was produced. The reason for this is that the pace out sensor was triggered twice in the process of folding the ladder, this was subsequently corrected in the logging process. 

The other aspect of data validity is ``adequacy", ``is there sufficient amount of data variable to make valid predictions?". By examining data from one of the ladder machines producing no apparent output the question ``why", arises. In this case, the machine in question was jammed and the ladder machine could not deliver its output and hence stood still. An additional sensor would have enabled the predictive ability to identify why no output was produced. Further, decisions about the format of the final data sets and time granularity are also made at this phase. When addressing the data granularity, the maximum data sample rate is ``1 second", however, the data set shows that more than one sensor status changed within the limited time (see row 16 and 17 in Table~\ref{example}). At 09:56:31 row 16 shows that the pace in and pace out sensors both have values ``1". At 09:56:32 no sensor changed state hence no registration was recorded. At 09:56:33 row 17 shows that the pace in and pace out sensors both have values ``0", which means that multiple sensors changed state within the granularity of 1 second. Based on this observation, and as it is important to know if \emph{PaceIn} follows \emph{PaceOut} or \emph{PaceOut} follows \emph{PaceIn} in order to form a relation the used method of checking/recording sensors status at a granularity of 1 second is not sufficient. A finer granularity, such as 500 milliseconds or even finer is required.

Another aspect of the data preprocessing phase is to perform descriptive statistical analysis and visualization. The focus of this paper is on statistical analysis rather than data cleansing. Hence, only descriptive analysis and visualization are further discussed.


\subsection{Descriptive Statistics and Visualization }
\label{sec:dataanalysis}
Descriptive data analysis is primarily a graphic approach that provides a first insight into the data. The two main characteristics of descriptive analysis are \emph{honesty} and \emph{trust}. Honesty means that the data scientist should be open to all possibilities prior to exploring the data, whereas, trust means that the impression, data is making, is not deceiving. There are no formal/standard set of rules that can be used in descriptive analysis, however, common approaches are: summary statistics, correlation, visualization and aggregation. Summary statistics or univariate analysis is the first step that helps us to understand data. Univariate analysis is the simplest form of data analysis where the data being analyzed contains only one variable. Further, data correlation or multivariate analysis helps us to find relationships between two or more variables. Finding connections between variables also has a crucial impact on choosing and building the predictive model(s). 

\begin{table}[ht]
\caption{Univariate analysis}
\label{example3}
\centering
\begin{tabular}{cccccccc}
\hline\noalign{\smallskip}
 & MachineOn & PaceIn & PaceOut & FaultyString &  ScrewError & Alarm\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Mean & 0.98 & 0.36 & 0.12 & 0.03 & 0.08 & 0.51\\
Std. Deviation & 0.15 & 0.48 & 0.16 & 0.15 & 0.27 & 0.49\\
Minimum & 0 & 0 & 0 & 0 & 0 & 0 \\
Maximum & 1 & 1 & 1 & 1 & 1 & 1\\
Skewness & -6.59  & 0.62 & 1.21  & 6.18   & 3.08 & -0.49\\
Kurtosis & 41.50  & -1.61 & -0.53  & 36.24  & 7.53 & -1.99\\
\hline
\end{tabular}
\end{table}

 \begin{figure}
\centering
    \includegraphics[width=0.55\textwidth]{heatmap}
  \caption{Sensor and alarm correlation heat map}
  \label{fig:correlation1}
\end{figure}

Data visualization helps us to gain perspective into the data, such as to find anomalies or to detect outliers. Finally, data aggregation helps us to group data from coarser to finer granularities in order to improve understanding due to the limited nature of data. Table~\ref{example3} (univariate descriptive analysis) shows mean, standard deviation, minimum, maximum, skewness and kurtosis. The most interesting findings in Table~\ref{example3} are skewness and kurtosis. Skewness is a measure of symmetry and kurtosis is a measure of  tailedness. Table~\ref{example3}, illustrates that \emph{MachineOn} variable is extremely skewed towards right side (98\% of the rows shows that machine is on). \emph{FaultyString} and \emph{ScrewError} variables are also extremely skewed towards left. Similarly, \emph{MachineOn} and \emph{FaultyString} variables have very high positive Kurtosis values that means that \emph{MachineOn} is substantially peaked towards 1 and \emph{FaultyString} is peaked towards 0. As, for perfectly symmetrical data the skewness is 0 and kurtosis is 3 for that reason it can be concluded that at least half of the variables of the machine data  are highly skewed/peaked towards either 1 or 0.

\begin{figure}
\centering
    \includegraphics[width=0.75\textwidth]{heatmap222}
  \caption{Sensor and alarm (aggregated at daily granularity) correlation heat map \cite{nadeem}}
  \label{fig:correlation2}
\end{figure}

In addition, correlation matrices are constructed to carry out a multivariate descriptive analysis. The correlation matrix of the sensor and the alarm variables at 1 second granularity (Fig.~\ref{fig:correlation1}) shows no interdependence. For that reason, data is being aggregated at daily granularity by job. Fig.~\ref{fig:correlation2}, shows some interesting positive and negative correlations. The correlations with respect to \emph{PaceIn}, \emph{PaceOut}, \emph{ScrewError}, \emph{FaultyString}, \emph{MachineOff}, number of unplanned \emph{MachineStops} and \emph{DownTime} are of particular interest. Due to the fact that one of the main aims of this analysis is to figure out which factors negatively impact the production and/or triggers machine stoppage. The correlation coefficient value normally locates between -1 and +1. The coefficient values between \emph{PaceIn}/\emph{PaceOut} and \emph{FaultyString}/\emph{MachineStops} (-0.35 and +0.37) indicate both weak negative and positive correlations. Further, the coefficient values between \emph{ScrewError}/\emph{FaultyString} and \emph{MachineStops} (+0.37 and +0.38) indicate weak positive correlations.  Moreover, the coefficient values  between \emph{DownTime} and \emph{JobDuration}/\emph{MachineOff} duration (+0.54 and +0.96) indicate moderate to strong positive correlations. Hence, it can be concluded that \emph{ScrewError} and \emph{FaultyString} both have weak to moderate effect on the number of unplanned \emph{MachineStops}, however, the duration of these stops have a strong positive correlation with machine \emph{DownTime}. 

\begin{figure}
\centering
\includegraphics[width=0.94\textwidth]{machinestatus2} 
\caption{Machine, faulty string and screw error status}
\label{fig:status}
\end{figure}


 \begin{figure}
\centering
\includegraphics[width=0.76\textwidth]{sensordata22} 
\caption{Sensor and alarm data overview \cite{nadeem}}
\label{fig:sensordataoverview}
\end{figure}

The status of the machine, screw errors and faulty PARs can also be viewed in Fig.~\ref{fig:status} (a-c). It is quite obvious that screwing machine errors and faulty PARs are causing a high percentage of machine stoppages (white holes in the data set). Further, Fig.~\ref{fig:sensordataoverview} (a-d) provide an overview of the sensor and alarm data at hourly and daily granularities, respectively. It is seen in Fig.~\ref{fig:sensordataoverview} (a) that the machine is on almost all the time. The pace of the incoming PARs is also fine showing very few stops, however, the pace of outgoing ladders show some stops. The outgoing pace slows down (Fig.~\ref{fig:sensordataoverview} (c)) between 07:15 and 07:20 as well as between 07:45 and 07:55. These slow downs are partly caused by errors in the screwing machine, and additionally these slow downs trigger the alarm. Similarly,  Fig.~\ref{fig:sensordataoverview} (b) demonstrates that the machine is on most of the time, incoming pace slows down between 16:00 and 21:00 mainly due to faulty PARs that also slow down the outgoing pace (Fig.~\ref{fig:sensordataoverview} (d)).

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{PaceINOUT444} 
\caption{Detailed data analysis at daily granularity \cite{nadeem}}
\label{fig:detailedanalysis}
\end{figure}

Moreover, the results of the detailed analysis at daily granularity are illustrated in Fig.~\ref{fig:detailedanalysis} (a-b). Fig.~\ref{fig:detailedanalysis} (a), shows that there are opportunities both for undertaking more jobs as well as for improving the machine on duration. Likewise, machine off duration and downtime are also quite significant. Screwing machine errors are little more frequent than faulty PARs and noticeably the alarm duration is also quite high. Fig.~\ref{fig:detailedanalysis} (b) presents the frequency of products produced, screwing machines errors, faulty PARs, alarms and stops. The frequency of the screwing machine errors, the alarms and the machine stops are noticeable. Further, Fig.~\ref{fig:pace} (a-b) reveals the pace of  incoming PARs and outgoing ladders. Fig.~\ref{fig:pace} (a), exhibits the ideal incoming and outgoing pace of the two main categories of ladders. The ideal average incoming pace is 9.5 seconds, whereas, the ideal average outgoing pace is 60 seconds. In addition, Fig.~\ref{fig:pace} (b), displays the actual incoming and outgoing pace at daily level. The actual average incoming pace is 15.5 seconds and the actual outgoing pace is 93.5 seconds. Hence, there is a clear possibility of optimizing both the incoming and outgoing pace.

\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{PaceINOUT333} 
\caption{In and out pace}
\label{fig:pace}
\end{figure}

To summarize, the descriptive analysis discloses that data is not uniformly distributed and almost half of the variables are highly skewed. Moreover, due to the binary nature of data, correlation matrices only reveal weak interdependence between the variables. In addition, visualisation and aggregations confirm that screwing machines errors are causing more machine stops than faulty PARs and that machine downtime needs to be reduced. In addition, to give these findings a commercial value a learning loop must be introduced where the finding are followed by actions and new data is compared to ``old" data to check if actions have the anticipated effect.
  


 

\section {Predictive Analysis}
\label{sec:modeling}
This section introduces the basic concepts of machine learning based models, data aggregation and explains some of the key issues such as imbalanced data. One of the main goals of this case study is ``to predict the machine's unplanned stops depending on historical consequences/patterns". Based on the kind of data available and the research question/goal, supervised machine learning is used to predict when the machine is going to stop. Supervised learning algorithms train from historical data, such as machine is on ``1" or off ``0".  The algorithm determines which label should be given to new data based on historical patterns. Most commonly used classification algorithms in machine learning are logistic regression, artificial neural networks, support vector machines (SVM), k-nearest neighbors (KNN), decision trees, auto-regressive integrated moving average (ARIMA), naive bayes and others \cite{gooijer}. In this paper, logistic regression, support vector machines, k-nearest neighbors, decision trees and artificial neural networks are used, however, due to space limitation only logistic regression and neural networks are further explained.


\subsection {Machine Learning based Models}

\subsubsection {Logistic Regression}
\label{sec:LR}
Logistic regression is one of the frequently used algorithm for binary classification. It predicts the binary (0 or 1) outcome by computing its probability.

The following set of equations present the logistic model for binary data:

\begin{equation}
\label{eq:eq10}
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ..... + \beta_n X_n
\end{equation}

Equation~\ref{eq:eq10}, is a linear regression equation, where $y$ is dependent variable and $X_1$, $X_2$ $...$ and $X_n$ are explanatory variables. $\beta_0$ is the intercept and  $\beta_1$, $\beta_2$  ... and  $\beta_n$ represent the slope of the regression line.

\begin{equation}
\label{eq:eq11}
p = 1/(1 + e^{-y})
\end{equation}

The logistic function presented in Equation~\ref{eq:eq11} is the sigmoid function. The sigmoid function is a mathematical function having an ``S" shaped curve (sigmoid curve). The logistic function restricts the probability ($p$) value between zero and one.

\begin{equation}
\label{eq:eq12}
p = 1/(1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ..... + \beta_n X_n)})
\end{equation}

Finally, Equation~\ref{eq:eq12} is applying Sigmoid function on the linear regression.

\subsubsection {Artificial Neural Networks}
\label{sec:NW}
Artificial neural networks or neural networks consist of algorithms that work in the same way as nerve cells or neurons in human brain. Neural networks are commonly used to recognize patterns and suites well for binary classification. Similar to logistic regression, neural networks also use activation function to generate a probability in the range of 0 to 1 in order to predict the outcome. 

The following set of equations present the neural network model for binary data:

\begin{equation}
\label{eq:eq15}
z_j = \sum_{i=1}^{n} (w_1,_ix_i + b_i),  1 \leq j \leq m
\end{equation}

Equation~\ref{eq:eq15}, is a summation equation, where $m$ represents the number of neurons in the hidden layer, $n$ indicates the number of inputs, $x_i$ represents the sensor values, $w_1,_i$ represents the first set of weights and $b_i$ is the bias value that intends to adjust the output.

\begin{equation}
\label{eq:eq13}
h_j = f(z_j) = max(0, z_j)
\end{equation}

The hidden layer activation function presented in Equation~\ref{eq:eq13} is the ReLU function, where $z_j$ is the input to a neuron and $m$ is the number of neurons in the hidden layer. The ReLU function is a mathematical function that stands for rectified linear unit. The activation function applies a ReLU function in order to restrict the $z_j$ value between zero and infinity. The ReLU is half rectified (from bottom), which means that $f(z_j)$ is zero when $z_j$ is less than zero and $f(z_j)$ is equal to $z_j$ when $z_j$ is above or equal to zero.

\begin{equation}
\label{eq:eq16}
y = \sum_{i=1}^{m} w_2,_ih_i
\end{equation}

Equation~\ref{eq:eq16}, is a summation equation, where $m$ indicates the number of neurons in the hidden layer, $h_i$ represents the intermediate results of the neurons in the hidden layer and $w_2,_i$ represents the second set of weights.

\begin{equation}
\label{eq:eq14}
f(y) = 1/(1 + e^{-y})
\end{equation}

The output layer activation function presented in Equation~\ref{eq:eq14} is the sigmoid function in order to restrict the final output value between zero and one.

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth]{neuralnetwork1} 
\caption{Single hidden layer artificial neural network}
\label{fig:neuralnetwork}
\end{figure}

Further, a single hidden layer neural network for the sensor binary data is presented in Fig.~\ref{fig:neuralnetwork}. Where, the input layer consists of the sensor values, such as pace in, pace out and so on. The hidden (middle) layer, uses Rectified Linear Unit (ReLU) activation function. The purpose of  ReLU is to calculate the intermediate result values for each node in the hidden layer (Equation~\ref{eq:eq13}). The input to the ReLU activation function is the sum of the product of the input sensor values with the first set of weights and the output is the intermediate result values. Finally, the output layer, uses Sigmoid activation function to determine the final output value. The input to the Sigmoid activation function is the sum of the product of the hidden layer intermediate result values with the second set of weights and the output is the final output value. Initially, the sets of weights are selected randomly by using Gaussian distribution (forward propagation) and afterwards backward propagation computes the margin of error of the final output value and update the sets of weights. 



\subsection {Data Aggregation}
\label{sec:aggregation}
\iffalse

When working with time-series forecasting we often have to choose between a few potential models and the best way is to test each model in pseudo-out-of-sample estimations. In other words, we simulate a forecasting situation where we drop some data from the estimation sample to see how each model perform.

A group of sensor values in a fixed sized window (15 consecutive sensor readings) are taken and convert them to an aggregated outcome in an effort to normalize the data. This pattern is then mapped into memory. Next, the current pattern is compared to all previous patterns. The pattern is compared to all the previous patterns. If their percent similarity is more than a certain threshold, then it will be considered. Afterwards, move forward one reading (row) and re-map the pattern. From here, 20-30 comparable patterns from history can be gathered. With these similar patterns, we can then aggregate all of their outcomes, and come up with an estimated "average" outcome. With that average outcome, if it is very favorable, then we might say that machine is working fine. If the outcome is not favorable, maybe we say that machine is going to “stop”.
\fi

\subsection {Imbalanced Data}
\label{sec:imbalanced}
In this subsection, four classification based supervised machine learning models are used to predict that whether the machine is on or off. Table~\ref{example3}, demonstrates the \emph{accuracy} of these models. Accuracy is a commonly used approach in classification models. It checks the percentage of correct predictions \cite{book}. The accuracy of the models with detailed data is 98\% and with aggregated data, it is 90\%, which means that the models are only predicting the majority class that is ``machine is on". As demonstrated in the descriptive analysis (Section~\ref{sec:datapreparation}) the data set is not uniformly distributed. 

\begin{table}[ht]
\caption{Predicted accuracy of the models on imbalanced data}
\label{example3}
\centering
\begin{tabular}{ccccc}
\hline\noalign{\smallskip}
 & Detailed data & Aggregated data \\

\noalign{\smallskip}
\hline
\noalign{\smallskip}
Logistic regression     &        98\% &90\%  \\
k-nearest neighbors    &         98\% & 90\% \\
Support vector machine         &        98\% & 91\%   \\
Decision tree &        98\% & 91\%   \\
\hline
\end{tabular}
\end{table}


Similarly, Fig.~\ref{fig:machineon2},  confirms that Machine on/off data is completely imbalanced. Out of 1.2 million instances only in 25,000 instances the machine is off. Even though the duration of these stops (downtime) is considerable (Fig.~\ref{fig:detailedanalysis} (a)), their frequency is still only 2\%. Meanwhile, the majority of the machine learning algorithms presume that the data samples are divided equally among the classes, causing the predictions for minority target class ``machine is off" to leads to incorrect results.

\begin{figure}
\centering
\includegraphics[width=0.36\textwidth]{machineon} 
\caption{Machine On/Off histogram}
\label{fig:machineon2}
\end{figure}





\subsection {Random Data Sampling}
\label{sec:sampling}
As the minority target class ``machine is off" is the main focus of prediction the data set has to be resampled. Two common approaches are \emph{oversampling} which is adding instances of the minority class ``machine is off" and \emph{undersampling} which is  deleting instances of the majority class ``machine is on" \cite{Gonzales}. In this paper, both undersampling and oversampling are used to adjust the class distribution. 

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{rocundersample} 
\caption{ROC curves (undersampled)}
\label{fig:machineon}
\end{figure}

Undersampling divides the machine on/off frequency into a 50/50 ratio that means, randomly selecting 25,000 instances where ``machine is on" and 25,000 instances where ``machine is off". Undersampling, dramatically reduces the sample size by discarding useful data, however, the predictions are not biased. Conversely, oversampling creates synthetic data from the minority class to reach an equal balance between the minority and majority classes. In oversampling, most of the data is retained since no rows are deleted. Oversampling increases the chances of \emph{overfitting} as it duplicates existing data. Overfitting occurs when a model can memorize the training data. Based on the memorization it makes correct predictions only if the test data is exactly the same as the training data. With all other datasets the predictions will be incorrect. 



Moreover, in order to decide which machine learning model(s) will be more effective in correctly identifying the status of the machine (on or off) in the random undersampling subset, \emph{receiver operating characteristic curve (ROC)} \cite{book} is used. ROC curve is a graph showing the performance of a binary classification model indicating how accurately the model separate between two events e.g. if a machine is on or off. Fig.~\ref{fig:machineon} demonstrate that in detailed and aggregated data in the random undersampling subset, logistic regression and support vector machine models achieve quite a good performance in distinguishing between the positive and the negative values. The ROC curve scores of these two models are 0.88 and 0.75, respectively. Later in this paper (Section~\ref{sec:evaluation}), a neural network is created and its accuracy is compared with one of the best models, logistic regression or support vector machine using as well undersampled as oversampled data.



\section {Evaluation}
\label{sec:evaluation}
\subsection{Overall Equipment Effectiveness}
In this section Dolle's manufacturing process is evaluated. This is done by computing the Overall Equipment Effectiveness (OEE) of the manufacturing process. OEE calculates the percentage of manufacturing time that is actually productive. It can be used as a benchmark as well as a baseline. It is one of the most widely used standards to calculate productivity in manufacturing industry. In general, OEE consists of three factors, which are \emph{availability}, \emph{performance} and \emph{quality} \cite{oee}. Availability considers all the incidents that stop the planned production. Performance considers those events that causes the manufacturing process to run at less optimal speed. Where as, quality takes into consideration the part of the manufactured products that do not meet quality standards. An OEE score of 100\% means that the manufacturing is going along at a perfect pace, without any unplanned stops and producing only good quality products. 

\begin{table}[]
\label{tab:distance}
\centering
\caption{Dolle's data for the morning shift}
\begin{tabular}{p{5.7cm}|p{5.7cm}}
%\begin{tabular}{c|c}
\hline
\multicolumn{1}{l}{\textbf{Item}} &  \textbf{Data} \\ \hline
Shift length            & 510 minutes\\
Breaks           &  60 minutes\\
Downtime (planned/unplanned stops)            &   80 minutes\\
Ideal time to produce a single ladder   &   60 seconds\\
Total count  & 260 ladders\\
Reject count & 2 ladders\\

\hline
\end{tabular}
\label{tab:data}
\end{table}

Based on the items and data mentioned in Table~\ref{tab:data} the OEE can be computed. The OEE will provide a clear picture of the productivity and possibilities for improvements. In order to calculate OEE, the following steps are performed. First, \emph{Planned Production Time (PPT)} and \emph{Run Time (RT)} are calculated. The Planned Production Time is the standard shift time excluding the planned breaks, such as lunch/coffee breaks as well as shift change over time. The Run Time is the actual time of production excluding both the planned and unplanned stops, such as job/product switch over stops, stops caused by faulty string or by screwing machine error and so on. Afterwards, \emph{Good Count (GC)} is calculated by rejecting the defected ladders.\\


\emph{PPT: Shift Length - Breaks = 510 minutes - 60 minutes = 450 minutes}\\

\emph{RT: PPT - Stop Time = 450 minutes - 80 minutes = 370 minutes}\\

\emph{GC: Total Count - Reject Count = 260 ladders - 2 ladder = 258 ladders} \\


Next, \emph{Availability (A)}, \emph{Performance (P)} and \emph{Quality (Q)} are to be calculated. Availability is the time when the manufacturing process is not running or the machine is ``OFF'' for some reasons. Availability takes machine failure (unplanned stops) and setup for next job and/or adjustments (planned stops) into account. Performance, estimates that whether the process is running at its optimal pace and quality concerns with quality standards of the products being produced.\\


\emph{A: RT / PPT = 370 minutes / 450 minutes = 0.8222 = 82.22\%}\\

\emph{P: (Ideal Production Time * Total Count) / RT} = 

\hspace{12mm} \emph{(60 seconds * 260 ladders)/(370 * 60 seconds) = 0.7027 = 70.27\%}\\

\emph{Q: Good Count / Total count = 258 ladders/ 260 ladders = 0.9923 = 99.23\%}\\

Finally, the OEE score is computed by multiplying the availability, performance and quality.\\ 

\emph{OEE: A * P * Q = 0.8222 * 0.7027 * 0.9923 = 0.5733 = 57.33\%}\\

An OEE score of 57.33\% is fairly typical for automate manufacturing industry, however, it indicates there is significant opportunity for improvement in performance. The performance score can be improved by reducing the switch over time between the jobs, by identifying the reasons for machine stops and finally by tackling the major cause(s) of downtime and so on. An OEE score around 85\% is considered a world class for automate manufacturing, where as, an OEE score of 40\% or below is considered low. For better understanding, OEE data for four sequential weeks is computed. Table~\ref{tab:dist}, shows an average OEE number (61\%) that captures, how well Dolle is doing and the three numbers that summarize the primary types of losses (Availability, Performance, and Quality). To conclude, an average OEE score of 61\% is fairly typical for automate manufacturers, however, it shows that there is considerable scope for improvement, specially the performance loss factor draws attention.



\begin{table}[]
\label{tab:distance}
\centering
\caption{OEE data for four sequential weeks}
\begin{tabular}{p{2.4cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}}
\hline
\multicolumn{1}{l}{\textbf{}} &  \textbf{Week 1} & \textbf{Week 2} & \textbf{Week 3}& \textbf{Week 4} & \textbf{Average} \\ \hline
OEE             & 59\%& 56\% &  63\% & 66\% & 61\%\\
Availability             & 90\%& 89\% &  91\% & 90\% & 90\%\\
Performance            & 66\%& 63\% &  71\% & 75\% & 68.75\% \\
Quality             & 99\%& 98\% &  97\% & 99\% & 98.50\%\\
\hline
\end{tabular}
\label{tab:dist}
\end{table}


\subsection{Testing of Machine Learning Models}

\iffalse

In this article, we show that logistic regression andartificial neural networks share common roots in sta-tistical pattern recognition, and how the latter modelcan be seen as a generalization of the former. 

We brieflycompare these two methods with other popular classification algorithms from the machine learning field, suchask-nearest neighbors, decision trees, and support vec-tor machines.



ogistic re-gression (LR) and artificial neural networks (ANN).These models have their origins in two different com-munities (statistics and computer science), but sharemany similarities. sta-tistical pattern recognition

This section measures the performance in terms of accuracy, precision and recall (Table~\ref{tab:evaluation}) of the chosen classification technique - logistic regression. Accuracy means that ``how often is the classifier correct?" Thus, the proposed algorithm has an accuracy of 80\%, that is, for each 100 ``machineOn = 1" or ``machineOn = 0" types it classified, 80 were correctly classified. Precision means that ``when the model predicts the positive result ``MachineOn = 1", how often is it correct?"
\fi

\iffalse


\begin{table}[]
\label{tab:distance}
\centering
\caption{Model evaluation}
\begin{tabular}{p{2.4cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}}
\hline
\multicolumn{1}{l}{\textbf{}} &  \textbf{Scores} \\ \hline
Accuracy      &        80\% \\
Precision     &         95\%\\
Recall         &        64\%\\
\hline
\end{tabular}
\label{tab:evaluation}
\end{table}

\fi





\iffalse

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{detailedddata2} 
\caption{confusion matrices detailed data (undersampling)}
\label{fig:confusion}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{aggregateddata} 
\caption{confusion matrices aggregated data (undersampling)}
\label{fig:confusion}
\end{figure}

\fi

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{confustionmatrixNW} 
\caption{confusion matrices}
\label{fig:confusion}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{accuracyscorelogisticregression} 
\caption{Accuracy scores}
\label{fig:confusion}
\end{figure}

\iffalse
Further, Fig.~\ref{fig:confusion} presents a confusion matrix using heat map. The matrix shows that out of 7660 actual instances (first row) of ``MachineOn = 0" (true negative), the classifier predicted correctly 7400 (96\%) of them. Similarly, out of 8030 instances (second row) of ``MachineOn = 1" (true positive), the classifier predicted correctly 5140 (64\%) of them. In the heat map, diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. Finally, Fig.~\ref{fig:auc} demonstrates a receiver operating characteristics curve (ROC) or area under the curve (AUC), which is commonly used to determine the performance of the classification algorithm. If the AUC score is close to 0.5, the classifier is just doing the random predictions, however, it predicts better as the score approaches close to 1. The AUC score of the proposed classifier is 0.87. It means that the classifier is quite decent at minimizing false negatives (``MachineOn = 1" which is classified as ``MachineOn = 0") and true negatives (''MachineOn = 0" which is classified as ``MachineOn = 0").
\fi




\section{Related Work}
\label{sec:relatedwork}
In general, smart manufacturing covers a number of different technologies, such as, data processing or data analytics, connected devices as well as services, robotics and so on. 

This section mainly concentrates on the previous work done in relation to predictive data analytics for smart manufacturing. According to Lee et al. \cite{Lee}, smart manufacturing still lacks smart analytical techniques and tools. In order to improve productivity, performance of the manufacturing machinery should be measured and optimized with the help of data analytics technologies. A state-of-the-art review of deep learning techniques for machinery fault diagnosis, predictive analytics and defect prognosis is presented by \cite{Wang}. Similarly, big data analytics in semiconductor manufacturing industry was studied by \cite{Moyne}. Further, M\"uller et al. \cite{Muller}, described that big data analytical assets are associated with an average of 3-7 \% improvement in company productivity. Tao et al. \cite{Tao}, mentioned that data analytics provides an opportunity in the manufacturing industry to adopt data-driven strategies in order to become more competitive. Further, a survey by Kamble et al. \cite{Kamble}, highlighted that the manufacturing industry has realized that the data analytics capabilities are must for future growth. Moreover, Auschitzky et al. \cite{Auschitzky}, proposed the use of advanced analytics such as, data visualization, correlation analysis and artificial neural networks to take a deep dive into historical data, in order to identify initial patterns. Predicting the bottlenecks in a production system based on the active periods of the machines using auto-regressive integrated moving average (ARIMA) method was proposed by \cite{Subramaniyan}. Similarly, a big data analytical architecture for product life cycle management as well as cleaner manufacturing was presented by \cite{Zhang}. Furthermore, Shin et al. \cite{Shin}, presented an analytic model for predicting energy consumption of manufacturing machinery. Chen et al. \cite{Chen}, proposed a Quality of Service (QoS) to manage data traffic using deep learning in the small and medium sized industry. Moreover, Candanedo et al. \cite{Candanedo}, applied machine learning models such as, logistic regression and random forest to predict equipment performance using the historical data set. These works focus on various aspects and recent advancements of data analytics technologies/techniques in smart manufacturing. The work presented in this paper is build on top of the ideas presented in those works. Most of them focus on theoretical rather than practical issues in relation to storage, management, processing and prediction of machine-related data, while the focus of this paper is to provide a practical application of the selected data analysis and machine learning techniques.

As, it can be seen from the above mentioned literature, sensor data analysis in manufacturing industry remains briefly addressed, for that reason this paper is among the very few featuring an in-depth sensor data analysis of imbalanced data using machine learning in order to enhance operational efficiency for small and medium sized enterprises (SME) in the manufacturing industry based on the real world case study.

\section{Conclusions and Future Work}
\label{sec:conclusionandfuturework}
This paper described the data (descriptive statistics analysis) and machine learning techniques built around the concept of industry 4.0. Descriptive statistics helped to describe and understand the features of data such as, correlation, skewness, kurtosis, class distribution and related. This paper also revealed that descriptive analysis is necessary to build effective machine learning models. Further, various machine learning algorithms such as, logistic regression, neural networks, support vector machines, decision trees and k-nearest neighbors were applied on a historical data set to predict costly production line disruptions. The accuracy of the proposed machine learning models were tested on a real-world data set. The results have validated the effectiveness of the proposed models.

For the future work, a near real-time anomaly detection mechanism using the machine learning models presented in this paper will be developed that can detect events that fail to match an expected pattern. In addition, a near real-time dashboard will be developed to display the input pace, output pace, screw errors, faulty PARs, OEE score and more.



\section{Acknowledgement}
This  research is supported  by University College of Northern Denmark - Research and Development funding and Dolle A/S.

\begin{thebibliography}{8}

\bibitem{industry}
Luz Mart\'in-Pe\~na, M., D\'iaz-Garrido, E., S\'anchez-L\'opez, J. M.: The Digitalization and Servitization of Manufacturing: A Review on Digital Business Models. Strategic Change, \textbf{27}(2), 91--99 (2018)

\bibitem{CRM}
Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., Wirth, R.:
CRISP-DM 1.0: Step-by-step Data Mining Guide. Technical Report. The CRISP-DM Consortium (2000)

\bibitem{Dolle}
Dolle, \url{http://www.dolle.eu}. Last accessed 11 Nov. 2019


\bibitem {nadeem}
Iftikhar, N., Andersen, T. B., Nordbjerg, F. E., Bobolea, E., Radu, P. B.: Data Analytics for Smart Manufacturing: A Case Study. In 8th International Conference on Data Science, Technology and Applications, pp. 392-399. SCITEPRESS (2019)


\bibitem{oee}
Overall Equipment Effectiveness (OEE), \url{https://www.oee.com}. Last accessed 13 Feb. 2019


%\bibitem {bigdata}
%Ward, J. S., Adam, B.: Undefined by Data: A Survey of Big Data Definitions. arXiv Preprint arXiv:1309.5821 (2013)


\bibitem{iftikhar}
Iftikhar, N., Liu, X., Nordbjerg, F. E.: Relational-Based Sensor Data Cleansing. In East European Conference on Advances in Databases and Information Systems, pp. 108-118. Springer, Cham (2015)

\bibitem{gooijer} De Gooijer, J. G., Hyndman, R. J.:  25 Years of Time Series Forecasting, International Journal of Forecasting, \textbf{22}(3), 443--473 (2006)

\bibitem{book}
Deitel, P.J., Dietal, H.: Intro to Python for Computer Science and Data Science: Learning to Program with AI, Big Data and the Cloud. Pearson Education, Incorporated (2020)

\bibitem{Gonzales} 
Gonz\`alez, S., Garc\'ia, S., Li, S. T., Herrera, F.: Chain based Sampling for Monotonic Imbalanced Classification. Information Sciences, 474, 187-204 (2019)

\bibitem{nw}
Dreiseitl, S., Ohno-Machado, L.: Logistic Regression and Artificial Neural Network Classification Models: A Methodology Review, Journal of Biomedical Informatics, \textbf{35}(5-6), 352--359 (2002)

\bibitem{Lee} 
Lee, J., Kao, H.-A., Yang, S.: Service Innovation and Smart Analytics for Industry 
4.0 and Big Data Environment. Procedia CIRP. 16, 3--8 (2014)

\bibitem{Wang} 
Wang, J., Ma, Y., Zhang, L., Gao, R. X., Wu, D.: Deep Learning for Smart Manufacturing: Methods and Applications. Journal of Manufacturing Systems (2018)

\bibitem{Moyne}
Moyne, J., Iskandar, J.: Big Data Analytics for Smart Manufacturing: Case Studies in Semiconductor Manufacturing. Processes \textbf{5}(3), 39--58 (2017)

\bibitem{Muller}
M\"uller, O., Fay, M., vom Brocke, J.: The Effect of Big Data and Analytics on Firm Performance: An Econometric Analysis Considering Industry Characteristics. Journal of Management Information Systems \textbf{35}(2), 488--50 (2018)

\bibitem{Tao}
Tao, F., Qi, Q., Liu, A., Kusiak, A.: Data-driven Smart Manufacturing. Journal of Manufacturing Systems (2018)

\bibitem{Kamble}
Kamble, S.S., Gunasekaran, A., Gawankar, S.A.: Sustainable Industry 4.0 Framework: A Systematic Literature Review Identifying the Current Trends and Future Perspectives. Process Safety and Environmental Protection \textbf{117}, 408--425 (2018) 

\bibitem{Auschitzky}
Auschitzky, E, Markus, H., Agesan R.: How Big Data can Improve Manufacturing. McKinsey \& Company 822 (2014)

\bibitem{Subramaniyan}
Subramaniyan, M., Skoogh, A., Salomonsson, H., Bangalore, P., Bokrantz, J.: A Data-Driven Algorithm to Predict throughput Bottlenecks in a Production System based on Active Periods of the Machines. Computers \& Industrial Engineering (2018)

\bibitem{Zhang}
Zhang, Y., Ren, S., Liu, Y., Si, S.: A Big Data Analytics Architecture for Cleaner Manufacturing and Maintenance Processes of Complex Products. Journal of Cleaner Production \textbf{142}, 626--641 (2017) 

\bibitem{Shin}
Shin, S. J., Woo, J., Rachuri, S.: Predictive Analytics Model for Power Consumption in Manufacturing. Procedia CIRP \textbf{15}, 153--158 (2014)

\bibitem{Chen} Chen, Z., Luo, L., Yang, H., Yu, J., Wen, M., Zhang, C.:  GENIE: QoS-guided Dynamic Scheduling for CNN-based Tasks on SME Clusters. In Design, Automation and Test in Europe Conference and Exhibition, pp. 1599-1602. IEEE (2019)

\bibitem{Candanedo} Candanedo, I.S., Nieves, E.H., González, S.R., Martín, M.T.S., Briones, A.G.: Machine Learning Predictive Model for Industry 4.0. In International Conference on Knowledge Management in Organizations pp. 501-510. Springer, Cham (2018)

%\bibitem{ref_article1}
%Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

%\bibitem{ref_lncs1}
%Author, F., Author, S.: Title of a proceedings paper. In: Editor,
%F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
%Springer, Heidelberg (2016). \doi{10.10007/1234567890}

%\bibitem{ref_book1}
%Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
%Location (1999)

%\bibitem{ref_proc1}
%Author, A.-B.: Contribution title. In: 9th International Proceedings
%on Proceedings, pp. 1--2. Publisher, Location (2010)

%\bibitem{ref_url1}
%LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
%Oct 2017
\end{thebibliography}
\end{document}
