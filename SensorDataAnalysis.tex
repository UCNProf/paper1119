% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{wrapfig}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Industry 4.0: Sensor Data Analysis using Machine Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Nadeem Iftikhar\inst{1} \and
Finn Ebertsen Nordbjerg\inst{1}\and
Thorkil Baattrup-Andersen\inst{2}\and
 Karsten Jeppesen\inst{1}}
%
\authorrunning{N. Iftikhar et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University College of Northern Denmark, Aalborg 9200, Denmark 
\email{\{naif,fen,kaje\}@ucn.dk}\\
 \and
Dolle A/S, Fr\o strup 7741, Denmark\\
\email{ta@dolle.com}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The technological revolution, known as industry 4.0, tends to improve efficiency/productivity and reduce production costs. In an Industry 4.0 based smart manufacturing environment, machine learning techniques are used to find patterns in newly arrived data by building models based on historical data. These models can then predict unseen consequences or issues. Initially, this paper performs a descriptive statistics and visualization, afterwards it addresses issues like classification of data with imbalanced class distribution. Further, several binary classification based machine learning models are built and trained for predicting production line disruptions, however, only logistic regression and artificial neural networks are discussed in detail. This paper evaluates the effectiveness of the machine learning models as well as the overall utilization of the manufacturing operation in terms of availability, performance and quality.















\iffalse
Due to the emergence of the fourth industrial revolution (Industry 4.0) or smart manufacturing, manufacturing business all over the world is changing dramatically; it needs enhanced efficiency, competency and productivity. More and more manufacturing machines are equipped with sensors and the sensors produce huge volume of data. Most of the companies do neither realize the value of data nor how to capitalize the data. The companies lack techniques and tools to collect, store, process and analyze the data. The objective of this paper is to propose data analytic techniques to analyze manufacturing/production data. The analytic techniques will provide both descriptive and predictive analysis. In addition, data from the company's ERP system is integrated in the analysis. The proposed techniques will help the companies to improve operational efficiency and achieve competitive benefits.
\fi
\keywords{Industry 4.0  \and Sensor Data \and Data Analysis \and Machine Learning \and Smart Manufacturing} \and {Imbalanced Data}
\end{abstract}
%
%
%
\section{Introduction}
The fourth industrial revolution (Industry 4.0) focuses greatly on automation, interconnected devices and sensors,  machine learning, data analysis and visualization. Industry 4.0 aims at enhancing productivity by increasing operational efficiency, development of new products, services and business models \cite{industry}. Data analysis uses many techniques ranging from statistics to machine learning and in the manufacturing industry, it can be used to identify production line interruptions, to optimize the manufacturing processes and so on in order to minimizes the downtime and maximize the productivity. The framework used in this paper to structure and execute the data analysis and modeling methods is Cross-industry Standard Process for Data Mining (CRISP-DM) \cite{CRM}. It consists of business objectives, data insight, data preprocessing, modeling and evaluation. This work is done in collaboration with Dolle \cite{Dolle}. Dolle is one of the front-runners in  manufacturing of timber loft ladders. It is established in more than 40 countries worldwide. To compete globally and to have an efficient production process, in addition to increase in quality, profitability and productivity has led the development of these sensor data analysis and machine learning techniques. One of the main goals of building machine learning methods for Dolle is to reduce downtime and cost by predicting production line disruptions.



\iffalse
 Where new technologies merge the physical, digital and biological spheres. Industry 4.0 requires no human involvement in manufacturing and depends on artificial intelligence, machine learning and big data technologies. In order to retain the prospective and competitive position in the international market and to optimize productivity, \textit{Dolle} \cite{Dolle} relies on business analytics to explore large volumes of data, expose undetected patterns, correlations and new key production parameters. Dolle is a market leader in Europe for wooden loft ladders. It is present in more than 40 countries worldwide, loft ladders remain its flagship product, however, it also produces modular staircases, spiral and space saving staircases as well as a comprehensive range of banisters under the \textit{Prova} brand. Further, industry proven Cross-industry Standard Process for Data Mining (CRISP-DM) \cite{CRM} is used for problem solving in Dolle's business analytics. It consists of following six phases: business understanding, data understanding, data preparation, modeling, evaluation and deployment (optional). 

This paper presents data analytic techniques capable of performing both descriptive and predictive analysis. In order to demonstrate the techniques, a real-world case study from manufacturing industry is selected. The sensor, alarm and enterprise resource planning (ERP) system data provided by the case study is first consolidated at a central repository. Then, an exploratory/descriptive analysis is performed in order to gain insight into the real business problems. Further, a predictive analysis using machine learning is performed. To summarize, the main contributions in this paper are as follow:
\fi
This paper is a significant extension of our previous conference paper \cite{nadeem}. In the previous work a data pipeline to handle ingestion, processing and analysis of data was proposed. An exploratory analysis of the data was provided. Further, a statistical-based machine learning model to predict costly production line disruptions was also presented using a real-life case study. This paper extends \cite{nadeem} by building multiple machine learning models and evaluating the performance of these models to select the best one(s). In addition, it also presents techniques to handle disproportion data.

To summarize, the main contributions in this paper are as follow:
\begin{itemize} 
\item Providing an in-depth descriptive statistics and visualization. 

\item Presenting techniques for handling imbalance data.

%\item It investigated how to uncover hidden patterns in the data.
\item Building multiple machine learning models for predicting costly production line disruptions.

%\item It implemented a near real-time dashboard that displays the input/output pace.
\item Comprehensive evaluation of the equipment effectiveness and the performance of the proposed models.
\end{itemize} 

The paper is structured as follows. Section~\ref{sec:businessunderstanding} describes the objectives and requirements from a business perspective. Section~\ref{sec:dataunderstanding} gives initial insights about data. Section~\ref{sec:datapreparation} provides descriptive statistics. Section~\ref{sec:modeling} presents
the machine learning models. Section~\ref{sec:evaluation} evaluates the equipment effectiveness and performance of the models. Section~\ref{sec:relatedwork} presents the related work. Section~\ref{sec:conclusionandfuturework} concludes the paper and points out the future research directions.

\section{Objectives}
\label{sec:businessunderstanding}
\iffalse
\begin{wrapfigure}{r}{0.4\textwidth}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{machine1}
  \end{center}
  \vspace{-3mm}
  \caption{Three-section timber loft ladder at Dolle's assembly line}
  \label{fig:ladder1}
\end{wrapfigure}
\fi

\iffalse
The focus of this section is to understand the basic concepts of smart manufacturing in consultation with domain experts. Project objectives are derived from the viewpoint of Dolle requirements and subsequently converted into data science problem definitions. Some of Dolle's primary objectives, from a business perspective are described as follow. When machines are started, Dolle would like to know how long it takes, before the right output pace with regards to the product (ladder) manufactured is achieved (see Fig.~\ref{fig:ladder1}). Output rate is the average time between the start of production of one unit and the start of production of the next unit. How fast are items moving through the machines? What is optimal rate? In addition, What are the causes of production disruption? Dolle would also like to know how much time is spent on changeovers (product types). A business goal states objectives in business terms, whereas, a data mining goal states objectives in technical terms. A non-exhaustive list of data mining goals:
\fi
The focus of this section is to understand the basic concepts of smart manufacturing in consultation with domain experts. The objectives of the project are derived from the viewpoint of Dolle and it's requirements. Hereafter, this is converted into data science problems. From a business perspective some of Dolle's primary questions are: When machines are started, how long does it take, before the right output pace is achieved (see Fig.~\ref{fig:ladder1})? The output pace is defined as the average time between the start of manufacturing of one unit and the start of manufacturing of the next unit. How fast are items moving through the machines? What is the optimal rate? What are the causes of production stops? In addition, Dolle would like to know the amount of time spent on changeovers from one product type to another. 

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{machine1}
\caption{Three-section timber loft ladder at Dolle's assembly line}
\label{fig:ladder1}
\end{figure}

After defining the goals in business terminology, these goals are then converted in technical terms, known as data mining goals. A non-exhaustive list of the data mining goals is presented below:
\begin{enumerate} 
\item Frequency of machine stops due to faulty strings/screw errors as well as total downtime of the machine due to faulty strings/screw errors?
\item How fast are items moving through the machine (production rate)?
\item What is the maximum pace and are there any delays in the pace?
\item Based on historical patterns, predict machine stops and/or how to prevent them?
\item What is the overall downtime of a machine and/or what are the costs?
\end{enumerate} 



\iffalse
In general, production with 80-85 \% efficiency is considered very efficient. It is of interest to look into every predicted and unpredicted issue/challenge during production. Why did it happen? Can it be predicted and if so can it be prevented or prepared for? How can production be optimised? Some challenges during production are known, such as, breakdowns, changeovers, minor stoppages, reduced speed, defects and setup scrap. As a result, the success of the manufacturing process can be measured by calculating the \textit{Overall Equipment Effectiveness (OEE)} \cite{oee}. OEE is the most widely used standard for measuring manufacturing productivity. 
\fi
In general, production with 80-85 \% efficiency is considered very efficient. It will be interesting to investigate every predicted and unpredicted problem during production: What caused it? Can it be predicted and if so, can it be prepared for or even prevented? Some of the known challenges during production are: breakdowns, changeovers, minor stoppage, reduced speed, defects and setup scrap. Hence, the quality of the manufacturing process can be measured by calculating the \textit{Overall Equipment Effectiveness (OEE)} \cite{oee}. OEE is the most common standard for measuring manufacturing productivity. It identifies the percentage of manufacturing time that is truly productive. 

\section{Initial Insights about Data}
\label{sec:dataunderstanding}
\iffalse
This section starts with data collection and proceeds with activities that targets understanding the data. These activities include first insight into the data, identifying data for analysis purposes, discovering data quality issues and/or detecting interesting subsets to form hypothesis regarding previously undetected patterns. Machine data (sensors and alarms) and ERP system data (product, job execution and work calendar) are provided by Dolle (Fig.~\ref{fig:data}). The machine data are provided in the form of binary values of 0's and 1's. The number of attributes depends on the machine in question. The product dataset consists of 85 attributes, the job execution dataset consists of 69 attributes and the work calendar dataset contains 10 attributes. Each job represents a specific business task that is carried out for a certain time interval to produce particular type of ladders. The structure of the data does not conform to any standard and additionally no assumptions can be made that two identical units or machines display identical structures. 
%This potentially presents a big data \cite{bigdata} problem, where data has 3 V’s: volume, velocity and variety. 
\fi
This section starts with  data collection and proceeds with activities that describe how initial insights about data are obtained. Firstly, the data collection is described and hereafter, activities that provide understanding of the data are discussed. These activities include getting the first insight into the data, identifying data for analysis purposes, discovering data quality issues and/or detecting interesting subsets to form hypothesis regarding previously undetected patterns. The data provided by Dolle is machine data and ERP system data (Fig.~\ref{fig:data}). Machine data describe the state of sensors and alarms, whereas, ERP data provides information about products, job executions and work calendar. The machine data consists of only binary values (0's and 1's). The number of attributes depends on the specific machine in question. The product dataset contains 85 attributes, the job execution dataset contains 69 attributes and the work calendar dataset has 10 attributes. Each job represents a specific business task that is carried out for a certain time interval to produce particular type of ladders. The structure of the data does not conform to any standard and additionally no assumptions can be made that two identical units or machines display identical structures. %This potentially presents a big data \cite{bigdata} problem, where data has 3 V’s: volume, velocity and variety.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{data} 
 \vspace{-7mm}
\caption{Data overview}
\label{fig:data}
\end{figure}

\begin{table*}[ht]
\caption{Sensor and alarm data \cite{nadeem}}
\label{example}
\centering
\begin{tabular}{c|ccccccc}
\hline\noalign{\smallskip}
\emph{Id} & DateTime & MachineOn & PaceIn & PaceOut &  FaultyString  & ScrewError & Alarm\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
\emph{.} & . & . & . & . & . & . & . \\
\textbf{\emph{1}} & \textbf{19-02-2019 09:53:07}  & \textbf{1} & \textbf{0} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\emph{2} & 19-02-2019 09:53:09  & 1 & 1 & 1 &  0 & 0 & 0\\
\emph{3} & 19-02-2019 09:53:10  & 1 & 0 & 1 &  0 & 0 & 0\\
\emph{4} & 19-02-2019 09:53:12  & 1 & 0 & 0 &  0 & 0 & 0\\
\emph{.} & . & . & . & . & . & . & . \\
\emph{5} & 19-02-2019 09:53:56  & 1 & 1 & 0 &  0 & 0 & 0\\
\emph{6} & 19-02-2019 09:53:58  & 1 & 0 & 0 &  0 & 0 & 0\\
\textbf{\emph{7}} & \textbf{19-02-2019 09:54:04}  & \textbf{1} & \textbf{0} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\emph{8} & 19-02-2019 09:54:09  & 1 & 0 & 0 &  0 & 0 & 0\\
\emph{9} & 19-02-2019 09:54:14  & 1 & 1 & 0 &  0 & 0 & 0\\
\emph{10} & 19-02-2019 09:54:15  & 1 & 0 & 0 &  0 & 0 & 0\\
\textbf{\emph{11}} & \textbf{19-02-2019 09:54:20}  & \textbf{1} & \textbf{0} & \textbf{0} &  \textbf{0} & \textbf{0} & \textbf{1}\\
\emph{12}& 19-02-2019 09:54:21  & 1 & 1 & 0 &  0 & 0 & 1\\
\emph{.} & . & . & . & . & . & . & . \\
\textbf{\emph{13}} & \textbf{19-02-2019 09:56:14}  & \textbf{1} & \textbf{0} & \textbf{0} &  \textbf{0} & \textbf{1} & \textbf{0}\\
\emph{14} & 19-02-2019 09:56:16  & 1 & 0 & 0 &  0 & 0 & 0\\
\textbf{\emph{15}} & \textbf{19-02-2019 09:56:29}  & \textbf{1} & \textbf{0} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\textbf{\emph{16}} & \textbf{19-02-2019 09:56:31}  & \textbf{1} & \textbf{1} & \textbf{1} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\textbf{\emph{17}} & \textbf{19-02-2019 09:56:33}  & \textbf{1} & \textbf{0} & \textbf{0} &  \textbf{0} & \textbf{0} & \textbf{0}\\
\emph{.} & . & . & . & . &  . & . & . \\
\hline
\end{tabular}
\end{table*}
\iffalse
Dolle's case study clearly illustrate the challenges faced in data analysis in the smart manufacturing industry. The data analysis methods presented in this paper, however, are general. In this case study machine data (from the production facility) has to be logged in order to register the states of the machines. The logged data is initially kept in detailed format in different database tables (a separate table for each machine). As mentioned above, each machine has a different set of sensors/attributes, for that reason only one of the machine is considered for demonstration purposes. The selected machine (\emph{Machine\_1}) consists of the following attributes: \emph{(DateTime, MachineOn, PaceIn, PaceOut, FaultyString, ScrewError, Alarm)}. The DateTime identifies a recording of a date and time event at one second granularity. The \emph{MachineOn} sensor indicates the machine is running for a given job. The \emph{PaceIn} or entrance of a string/beam sensor represents an incoming string. The \emph{PaceOut} or exit of a ladder sensor represents an outgoing ladder. The FaultyString sensor signifies the quality of the string, the bended or twisted strings are regarded as faulty strings. The ScrewError sensor corresponds to the screw machine that screw strings into place. Finally, the Alarm or Error sensor stands in for the general abnormality in the machine. For example, a string has struck in the machine. The alarm in the long run may lead the machine to a stop still.
\fi
Dolle's case study clearly illustrate the challenges faced in data analysis in the smart manufacturing industry. The data analysis methods presented in this paper, however, are general. In this case study machine data from the production facility are logged in order to record the states of the machines at any given time. The logged data is initially kept in detailed format in different database tables (a separate table for each machine). As mentioned above, each machine has a different set of sensors/attributes, for that reason only one of the machine is considered for demonstration purposes. The selected machine (\emph{Machine\_1}) consists of the following attributes: \emph{(DateTime, MachineOn, PaceIn, PaceOut, FaultyString, ScrewError, Alarm)}. The DateTime identifies a recording of a date and time event at a second granularity. The \emph{MachineOn} sensor indicates the machine is running for a given job. The \emph{PaceIn} or entrance of a beam/string sensor represents an incoming string. The \emph{PaceOut} or exit of a ladder sensor represents an outgoing ladder. The \emph{FaultyString} sensor signifies the quality of the string, the bended or twisted strings are regarded as faulty strings. The \emph{ScrewError} sensor corresponds to the screw machine that screw strings into place. Finally, the \emph{Alarm} or Error sensor represents a general abnormality in the machine. For example, a string has struck in the machine. The alarm in the long run may lead the machine to a stop still.

In order to provide a snapshot of data, a real machine dataset provided by Dolle is used. The snapshot contains 7 attributes for job no. 307810 to produce CF (ClickFix) type ladder. In Table~\ref{example}, initially the granularity of the detailed data is at \emph{second by job by machine}. For instance, row number 1 reads as follows: \emph{DateTime}=19-02-2019 09:53:07 (represents: second granularity. It is important to note that if the next row has same values as the previous row in that case the next row will not be logged to the database, for that reason the holes at the second granularity are visible), \emph{MachineOn}=1 (represents: machine is running), \emph{PaceIn}=0 (represents: no string is entering), \emph{PaceOut}=1 (represents: exiting of the ladder), \emph{FaultyString}=0 (represents: the quality of the string is OK), \emph{ScrewError}=0 (represents: no error in the screwing machine) and \emph{Alarm}=0 (represents: no abnormality in the machine). Whereas, Id is an abstract attribute and used only for row identification purposes. Similarly, row number 13 reads as follows: \emph{DateTime}=19-02-2019 09:56:14, \emph{MachineOn}=1, \emph{PaceIn}=0, \emph{PaceOut}=0 (represents: no ladder is exiting), \emph{FaultyString}=0, \emph{ScrewError}=1 (represents: an error in the screwing machine) and \emph{Alarm}=0 (represents: no abnormality in the machine identified yet). Further, initial look into the data in Table~\ref{example} reveals some interesting facts, such as, the ladder is produced (row 7) in 09:54:04 - 09:53:07 = 57 seconds (average pace), where as, the next ladder is produced (row 15) in 09:56:29 - 09:54:04 = 145 seconds. The delay in the production of the next ladder is due to the fact that an alarm has been triggered (row 11) and a screw machine error has also caused the delay (row 13). 

Further, Table~\ref{example2} displays some of the attributes of the ERP system data that may be used for analysis purposes \emph{(JobId, ProductId, JobStart (date and time), JobEnd (date and time), JobFinished, GoodQuality)}. As mentioned earlier, ERP system data has 164 attributes, in total. The \emph{JobId} represents the given job. The \emph{productId} represents a specific product. The \emph{JobStart} represents the date and time when a job starts. Similarly, \emph{JobEnd} represents the date and time when a job ends. A job may be carried out in multiple time intervals as seen in Table~\ref{example2}. The \emph{JobFinished} characterizes if the current job has finished and the \emph{GoodQuality} attribute represents the number of acceptable quality products produced. 

\begin{table}[ht]
\caption{ERP system data}
\label{example2}
\centering
\begin{tabular}{ccccccc}
\hline\noalign{\smallskip}
JobId & ProductId & JobStart & JobEnd &  JobFinished & GoodQuality\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
. & . & . & . &  .   \\
307810  & 524167 & 19-02-2019 09:34:23  & 19-02-2019 10:17:43   & 0 & 0\\
370810  & 524167 & 19-02-2019 10:28:28  & 19-02-2019 11:23:04   & 1 & 70\\
. & . & . & . &  .   \\
\hline
\end{tabular}
\end{table}

Furthermore, several interesting subsets may be identified from the initial insights about data that leads to hypothesis regarding initial data patterns. For example, whether screw machine errors causes more machine stops than faulty strings. 




 

\section {Data Preprocessing}
\label{sec:datapreparation}
\iffalse
This section provides insight into the business problems before performing data modeling. The data preparation phase include activities, such as data selection, data transformation, data cleaning and data validation. Data preparation tasks may be performed several times and not in any given order. During this phase important issues are addressed like selecting the relevant data, cleaning of data, discarding unacceptable data and how the ERP system data can be integrated into the final data sets. Some of the cleaning techniques discussed in \cite{iftikhar} may be applicable here as well. Metadata originating from discussions between data scientists and domain experts has shown great importance in the process of data validation. Some meta issues can not be inferred from the sensor data but require domain expertise like: ``is the machine output reliable, when the `error sensor or alarm' is `on', can this be verified?". Logically the answer is ``yes", as during production of certain types of ladders the alarm is disregarded. Another anomaly is that the output showed double the numbers of ladders produced that actually produced. The reason is that the pace out sensor was triggered twice in the process of folding the ladder, this was subsequently corrected in the logging process. 

The other aspect of data validity is ``adequacy", ``is there sufficient amount of data variable to make valid predictions?". By examining data from one of the ladder machines where no output was generated the question ``why", arises. In this case, the  machine in question was jammed and the ladder machine could not deliver it’s output and hence stood still. An additional sensor would have enabled the predictive ability to identify why no output was produced. 

Further, decisions about the format of the final data sets and granularity are also made at this phase. When addressing the data granularity, the maximum data sample rate is ``1 second", however, the data set shows that more than one sensor status changed within the limited time (see row 16 and 17 in Table~\ref{example}). It can seen in row 16 that the pace in and pace out sensors both have values ``1" at 09:56:31, as there is no change in the status of the sensors at 09:56:32 for that reason no row has been recorded. Similarly, row 17 shows that pace in and pace out sensors both have values ``0" at 09:56:33, which means that multiple sensors status changes within 1 second. Based on this observation, when trying to establish a relation it is important to know if \emph{PaceIn} follows \emph{PaceOut} or \emph{PaceOut} follows \emph{PaceIn}, hence the used method of checking/recording sensors status at a granularity of 1 second may not be a good option, it should be at a finer granularity, such as 500 milliseconds or better.
Another aspect of data preparation phase is to perform exploratory analysis. The focus of this paper is on exploratory data analysis rather than data cleansing or validation for that reason only exploratory analysis is further discussed.
\fi
This section provides insight into the business problems before performing data modeling. The data preprocessing phase include activities, such as data selection, data transformation, data cleaning and data validation. These tasks may be performed several times and not in any given order. During this phase important issues like selecting the relevant data, cleaning of data and discarding unacceptable data are addressed. In addition, it is investigated how the ERP system data can be integrated into the final data sets. Some of the cleaning techniques discussed in \cite{iftikhar} may be applicable here as well. Metadata originating from discussions between data scientists and domain experts has shown great importance in the process of data validation. Some meta issues cannot be inferred from the sensor data but require domain expertise like: ``is the machine output reliable, when the `error sensor or alarm' is `on', can this be verified?". Logically the answer is ``yes", as during production of certain types of ladders the alarm is disregarded. Another anomaly is that the logged data showed twice the numbers of ladders produced than actually was produced. The reason for this is that the pace out sensor was triggered twice in the process of folding the ladder, this was subsequently corrected in the logging process. 

The other aspect of data validity is ``adequacy", ``is there sufficient amount of data variable to make valid predictions?". By examining data from one of the ladder machines where no output was generated the question ``why", arises. In this case, the  machine in question was jammed and the ladder machine could not deliver it’s output and hence stood still. An additional sensor would have enabled the predictive ability to identify why no output was produced. Further, decisions about the format of the final data sets and time granularity are also made at this phase. When addressing the data granularity, the maximum data sample rate is ``1 second", however, the data set shows that more than one sensor status changed within the limited time (see row 16 and 17 in Table~\ref{example}). It can seen in row 16 that the pace in and pace out sensors both have values ``1" at 09:56:31, as there is no change in the status of the sensors at 09:56:32 for that reason no row has been recorded. Similarly, row 17 shows that pace in and pace out sensors both have values ``0" at 09:56:33, which means that multiple sensors status changes within 1 second. Based on this observation, when trying to establish a relation it is important to know if \emph{PaceIn} follows \emph{PaceOut} or \emph{PaceOut} follows \emph{PaceIn}, hence the used method of checking/recording sensors status at a granularity of 1 second may not be a good option, it should be at a finer granularity, such as 500 milliseconds or even finer.

Another aspect of the data preprocessing phase is to perform descriptive statistical analysis and visualization. The focus of this paper is on statistical analysis rather than data cleansing. Hence, only descriptive analysis and visualization are further discussed.

\iffalse
\subsection{Data Pipeline}
\label{sec:datapipeline}
The proposed data pipeline consists of digesting or processing raw data, extracting meaningful features and applying machine learning model. Fig.~\ref{fig:pipeline1} presents the data pipeline along with the proposed technologies. This pipeline is not specific to Dolle and may easily be adapted to other situations. At data acquisition step, raw data is collected from multiple data sources and stored at a central repository. Data acquisition can be performed with Python, R or Scala. Next, a data wrangling step transforms the data into a canonical data format that is used for further processing and analysis. Data cleaning, reduction and integration also takes place at this stage. Further, data exploration step performs initial descriptive analysis and visualization. For data wrangling and exploration, Python's \emph{pandas}, Scala's \emph{slick} and R's \emph{dplyr} are recommended technologies, where as, for visualization Python's \emph{matplotlib} and \emph{seaborn}, Scala's \emph{vegas} and R's \emph{ggplot2} are powerful technologies. Furthermore, data modeling step is the general concept of building a model that is capable of making predictions. For predictive modeling, Python's \emph{tensorflow} and \emph{scikit-learn}, Scala's \emph{spark mllib \& ml} and R's \emph{h2o} are well known technologies. In addition, the dashboard may displays the intput/output pace, the overall equipment effectiveness in near real-time. For dashboard, Python's \emph{dash} and \emph{boken}, Scala's \emph{flink} and R's \emph{shiny} are typical technologies.

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{pipeline2} 
\caption{Proposed data pipeline}
\label{fig:pipeline1}
\end{figure}

\fi

\subsection{Descriptive Statistics and Visualization }
\label{sec:dataanalysis}
Descriptive data analysis is primarily a graphic approach that provides a first insight into the data. The two main characteristics of descriptive analysis \emph{honesty} and \emph{trust}. Honesty means that the data scientist should be open to all possibilities prior to exploring the data, whereas, trust means that the impression, data is making, is not deceiving. There are no formal/standard set of rules that can be used in descriptive analysis, however, common approaches are: summary statistics, correlation, visualization and aggregation. Summary statistics or univariate analysis is the first step that helps us to understand data. Univariate analysis is the simplest form of data analysis where the data being analyzed contains only one variable. Further, data correlation or multivariate analysis helps us to find relationships between two or more variables. Finding connections between variables also has a crucial impact on choosing and building the predictive model(s). 

\begin{table}[ht]
\caption{Univariate analysis}
\label{example3}
\centering
\begin{tabular}{cccccccc}
\hline\noalign{\smallskip}
 & MachineOn & PaceIn & PaceOut & FaultyString &  ScrewError & Alarm\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Mean & 0.98 & 0.36 & 0.12 & 0.03 & 0.08 & 0.51\\
Std. Deviation & 0.15 & 0.48 & 0.16 & 0.15 & 0.27 & 0.49\\
Minimum & 0 & 0 & 0 & 0 & 0 & 0 \\
Maximum & 1 & 1 & 1 & 1 & 1 & 1\\
Skewness & -6.59  & 0.62 & 1.21  & 6.18   & 3.08 & -0.49\\
Kurtosis & 41.50  & -1.61 & -0.53  & 36.24  & 7.53 & -1.99\\
\hline
\end{tabular}
\end{table}

 \begin{figure}
\centering
    \includegraphics[width=0.6\textwidth]{heatmap}
  \caption{Sensor and alarm correlation heat map}
  \label{fig:correlation1}
\end{figure}

Data visualization helps us to gain perspective into the data, such as to find anomalies and to detect outliers. Finally, data aggregation helps us to group data from coarser to finer granularities in order to improve understanding due to the limited nature of data. Table~\ref{example3} (univariate descriptive analysis) shows mean, standard deviation, minimum, maximum, skewness and kurtosis. The most interesting findings in Table~\ref{example3} are skewness and kurtosis. Skewness is a measure of symmetry and kurtosis is a measure of  tailedness. Table~\ref{example3}, illustrates that \emph{MachineOn} variable is extremely skewed towards right side (98\% of the rows shows that machine is on). \emph{FaultyString} and \emph{ScrewError} variables are also extremely skewed towards left. Similarly, \emph{MachineOn} and \emph{FaultyString} variables have very high positive Kurtosis values that means that \emph{MachineOn} is substantially peaked towards 1 and \emph{FaultyString} is peaked towards 0. As, for perfectly symmetrical data the skewness is 0 and kurtosis is 3 for that reason it can be concluded that at least half of the variables of the machine data  are highly skewed/peaked towards either 1 or 0.

\begin{figure}
\centering
    \includegraphics[width=0.8\textwidth]{heatmap222}
  \caption{Sensor and alarm (aggregated at daily granularity) correlation heat map \cite{nadeem}}
  \label{fig:correlation2}
\end{figure}

In addition, correlation matrices are constructed to carry out multivariate descriptive analysis. The correlation matrix of the sensor and the alarm variables at second granularity (Fig.~\ref{fig:correlation1}) shows no interdependence. For that reason, data is being aggregated at daily granularity by job. Fig.~\ref{fig:correlation2}, shows some interesting positive and negative correlations. The correlations with respect to \emph{PaceIn}, \emph{PaceOut}, \emph{ScrewError}, \emph{FaultyString}, \emph{MachineOff}, number of unplanned \emph{MachineStops} and \emph{DownTime} are of particular interest. Due to the fact that one of the main aims of this analysis is to figure out which factors slow down the production and eventually triggers the machine to stop or stand still. The correlation coefficient value normally locates between -1 and +1. The coefficient values between \emph{PaceIn}/\emph{PaceOut} and \emph{FaultyString}/\emph{MachineStops} (-0.35 and +0.37) indicate both weak negative and positive correlations. Further, the coefficient values between \emph{ScrewError}/\emph{FaultyString} and \emph{MachineStops} (+0.37 and +0.38) indicate weak positive correlations.  Moreover, the coefficient values  between \emph{DownTime} and \emph{JobDuration}/\emph{MachineOff} duration (+0.54 and +0.96) indicate moderate to strong positive correlations. Hence, it can be concluded that \emph{ScrewError} and \emph{FaultyString} both have weak to moderate effect on the number of unplanned \emph{MachineStops}, however, the duration of these stops have a strong positive correlation with machine \emph{DownTime}. 

\begin{figure}
\centering
\includegraphics[width=0.94\textwidth]{machinestatus2} 
\caption{Machine, faulty string and screw error status}
\label{fig:status}
\end{figure}


 \begin{figure}
\centering
\includegraphics[width=0.80\textwidth]{sensordata22} 
\caption{Sensor and alarm data overview \cite{nadeem}}
\label{fig:sensordataoverview}
\end{figure}

The status of the machine, screw errors and faulty strings can also be viewed in Fig.~\ref{fig:status} (a-c). It is quite obvious that screw machine errors and faulty strings are causing a high percentage of machine stops (white holes in the data set). Further, Fig.~\ref{fig:sensordataoverview} (a-d) provide an overview of the sensor and alarm data at hourly and daily granularities, respectively. It is seen in Fig.~\ref{fig:sensordataoverview} (a) that the machine is on almost all the time. The pace of the incoming strings is also fine with very few stops, however, the pace of outgoing ladders has some stops. The outgoing pace slows down (Fig.~\ref{fig:sensordataoverview} (c)) between 07:15 and 07:20 as well as between 07:45 and 07:55. These slow downs are partly caused by errors in the screw machine, also both these slow downs trigger the alarm. Similarly,  Fig.~\ref{fig:sensordataoverview} (b) demonstrates that the machine is on most of the time, incoming pace slows down between 16:00 and 21:00 mainly due to faulty strings that also slow down the outgoing pace (Fig.~\ref{fig:sensordataoverview} (d)).

\begin{figure}
\centering
\includegraphics[width=0.77\textwidth]{PaceINOUT444} 
\caption{Detailed data analysis at daily granularity \cite{nadeem}}
\label{fig:detailedanalysis}
\end{figure}

Moreover, the results of the detailed analysis at daily granularity are illustrated in Fig.~\ref{fig:detailedanalysis} (a-b). Fig.~\ref{fig:detailedanalysis} (a), shows that there are opportunities both for undertaking more jobs as well as for increasing the machine on duration. Likewise, machine off duration and downtime are also quite significant. Screw machine errors are little more frequent than  faulty strings and definitely alarm duration is also quite high. Fig.~\ref{fig:detailedanalysis} (b) presents the frequency of products produced, screw machines errors, faulty strings, alarms and stops. The frequency of the screw machine errors, the alarms and the machine stops are noticeable. Further, Fig.~\ref{fig:pace} (a-b) reveals the pace of  incoming strings and outgoing ladders. Fig.~\ref{fig:pace} (a), exhibits the ideal incoming and outgoing pace of the two main categories of ladders. The ideal average incoming pace is 9.5 seconds, whereas, the ideal average outgoing pace is 60 seconds. In addition, Fig.~\ref{fig:pace} (b), displays the actual incoming and outgoing pace at daily level. The actual average incoming pace is 15.5 seconds and the actual outgoing pace is 93.5 seconds. Hence, there is a clear possibility of optimizing both the incoming and outgoing pace.

\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{PaceINOUT333} 
\caption{In and out pace}
\label{fig:pace}
\end{figure}

To summarize, the descriptive analysis discloses that data is not uniformly distributed and almost half of the variables are highly skewed. Moreover, due to the binary nature of data, correlation matrices only reveal weak interdependence between the variables. In addition, visualisation and aggregations confirm that screw machines errors are causing more machine stops than faulty strings and machine downtime needs to be reduced. In addition, to give these findings a commercial value a learning loop must be introduced where the finding are followed by actions and new data is compared to ``old" data to check if actions have the anticipated effect.
  


 












\section {Predictive Analysis}
\label{sec:modeling}






\iffalse

supervised approach
unsupervised approach
semi-supervised approach
Various forecasting methods will be used to predict patterns. Pre-dictive data analysis and modeling includes unsupervised and supervised machine learning, statistical in-ference, and prediction. A wide variety of machine learning algorithms are available, including k-nearest neighbors, naïve Bayes, decision trees, support vector machines, logistic regression, k-means, and so on. The choice of method to be deployed depends on the problem definition discussed in the first step and the type of collected data. The final product of this step is a model inferred from the data.

choose a model from the given approach
determine model assumptions
prepare input data
\fi

This section introduces the basic concepts of machine learning based models, data aggregation and explains some of the key issues such as imbalanced data. One of the main goals of this case study is ``to predict the machine's unplanned stops depending on historical consequences/patterns". Based on the kind of data available and the research question/goal, supervised machine learning is used to predict when the machine is going to stop. Supervised learning algorithms train from historical data, such as machine is on ``1" or off ``0".  The algorithm determines which label should be given to new data based on historical patterns. Most commonly used classification algorithms in machine learning are logistic regression, artificial neural networks, support vector machines (SVM), k-nearest neighbors (KNN), decision trees, auto-regressive integrated moving average (ARIMA), naive bayes and so on \cite{gooijer}. In this paper, logistic regression, support vector machines, k-nearest neighbors, decision trees and artificial neural networks are used, however, due to space limitation only logistic regression and neural networks are further explained.

%Moreover, frequently used classification algorithms in deep learning are convolutional neural networks (CNN), recurrent neural networks (RNN), artificial neural networks (ANN) and so on. Similarly, various sub-types within each of them also exists. In this paper, artificial neural networks are used for the reason that ANN performs better with binary classification.

%All the models perform classification by classifying the binary dependent variable into either zero or one.
\subsection {Machine Learning based Models}

\subsubsection {Logistic Regression}
\label{sec:LR}
Logistic regression is one of the frequently used algorithm for binary classification. It predicts the binary (0 or 1) outcome by computing its probability.

The following set of equations present the logistic model for binary data:

\begin{equation}
\label{eq:eq10}
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ..... + \beta_n X_n
\end{equation}

Equation~\ref{eq:eq10}, is a linear regression equation, where $y$ is dependent variable and $X_1$, $X_2$ $...$ and $X_n$ are explanatory variables. $\beta_0$ is the intercept and  $\beta_1$, $\beta_2$  ... and  $\beta_n$ represent the slope of the regression line.

\begin{equation}
\label{eq:eq11}
p = 1/(1 + e^{-y})
\end{equation}

The logistic function presented in Equation~\ref{eq:eq11} is the sigmoid function. The sigmoid function is a mathematical function having an ``S" shaped curve (sigmoid curve). The logistic function restricts the probability ($p$) value between zero and one.

\begin{equation}
\label{eq:eq12}
p = 1/(1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ..... + \beta_n X_n)})
\end{equation}

Finally, Equation~\ref{eq:eq12} is applying Sigmoid function on the linear regression.

\subsubsection {Artificial Neural Networks}
\label{sec:NW}
Artificial neural networks or neural networks consist of algorithms that work in the same way as nerve cells or neurons in human brain. Neural networks are commonly used to recognize patterns and suites well for binary classification. Similar to logistic regression, neural networks also use activation function to generate a probability in the range of 0 to 1 in order to predict the outcome. 

The following set of equations present the neural network model for binary data:

\begin{equation}
\label{eq:eq15}
z_j = \sum_{i=1}^{n} (w_1,_ix_i + b_i),  1 \leq j \leq m
\end{equation}

Equation~\ref{eq:eq15}, is a summation equation, where $m$ represents the number of neurons in the hidden layer, $n$ indicates the number of inputs, $x_i$ represents the sensor values, $w_1,_i$ represents the first set of weights and $b_i$ is the bias value that intends to adjust the output.

\begin{equation}
\label{eq:eq13}
h_j = f(z_j) = max(0, z_j)
\end{equation}

The hidden layer activation function presented in Equation~\ref{eq:eq13} is the ReLU function, where $z_j$ is the input to a neuron and $m$ is the number of neurons in the hidden layer. The ReLU function is a mathematical function that stands for rectified linear unit. The activation function applies a ReLU function in order to restrict the $z_j$ value between zero and infinity. The ReLU is half rectified (from bottom), which means that $f(z_j)$ is zero when $z_j$ is less than zero and $f(z_j)$ is equal to $z_j$ when $z_j$ is above or equal to zero.

\begin{equation}
\label{eq:eq16}
y = \sum_{i=1}^{m} w_2,_ih_i
\end{equation}

Equation~\ref{eq:eq16}, is a summation equation, where $m$ indicates the number of neurons in the hidden layer, $h_i$ represents the intermediate results of the neurons in the hidden layer and $w_2,_i$ represents the second set of weights.

\begin{equation}
\label{eq:eq14}
f(y) = 1/(1 + e^{-y})
\end{equation}

The output layer activation function presented in Equation~\ref{eq:eq14} is the sigmoid function in order to restrict the final output value between zero and one.

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth]{neuralnetwork1} 
\caption{Single hidden layer artificial neural network}
\label{fig:neuralnetwork}
\end{figure}

Further, a single hidden layer neural network for the sensor binary data is presented in Fig.~\ref{fig:neuralnetwork}. Where, the input layer consists of the sensor values, such as pace in, pace out and so on. The hidden (middle) layer, uses Rectified Linear Unit (ReLU) activation function. The purpose of  ReLU is to calculate the intermediate result values for each node in the hidden layer (Equation~\ref{eq:eq13}). The input to the ReLU activation function is the sum of the product of the input sensor values with the first set of weights and the output is the intermediate result values. Finally, the output layer, uses Sigmoid activation function to determine the final output value. The input to the Sigmoid activation function is the sum of the product of the hidden layer intermediate result values with the second set of weights and the output is the final output value. Initially, the sets of weights are selected randomly by using Gaussian distribution (forward propagation) and afterwards backward propagation computes the margin of error of the final output value and update the sets of weights. 

\iffalse

nadeem
\fi

\subsection {Data Aggregation}
\label{sec:aggregation}

When working with time-series forecasting we often have to choose between a few potential models and the best way is to test each model in pseudo-out-of-sample estimations. In other words, we simulate a forecasting situation where we drop some data from the estimation sample to see how each model perform.

A group of sensor values in a fixed sized window (15 consecutive sensor readings) are taken and convert them to an aggregated outcome in an effort to normalize the data. This pattern is then mapped into memory. Next, the current pattern is compared to all previous patterns. The pattern is compared to all the previous patterns. If their percent similarity is more than a certain threshold, then it will be considered. Afterwards, move forward one reading (row) and re-map the pattern. From here, 20-30 comparable patterns from history can be gathered. With these similar patterns, we can then aggregate all of their outcomes, and come up with an estimated "average" outcome. With that average outcome, if it is very favorable, then we might say that machine is working fine. If the outcome is not favorable, maybe we say that machine is going to “stop”.

\subsection {Imbalanced Data}
\label{sec:imbalanced}
Four classification based supervised machine learning models are used to predict that whether the machine is on or off. Table~\ref{example3}, demonstrates the \emph{accuracy} of these models. Accuracy is a commonly used approach in classification models. It checks the percentage of correct predictions. The accuracy of the models with detailed data is 98\% and with aggregated data, it is 90\%, which means that the models are only predicting the majority class that is ``machine is on". It is also demonstrated in the descriptive analysis (Section~\ref{sec:datapreparation}) that the data set is not uniformly distributed. 

\begin{table}[ht]
\caption{Predicted accuracy of the models on imbalanced data}
\label{example3}
\centering
\begin{tabular}{ccccc}
\hline\noalign{\smallskip}
 & Detailed data & Aggregated data \\

\noalign{\smallskip}
\hline
\noalign{\smallskip}
Logistic regression     &        98\% &90\%  \\
k-nearest neighbors    &         98\% & 90\% \\
Support vector machine         &        98\% & 91\%   \\
Decision tree &        98\% & 91\%   \\
\hline
\end{tabular}
\end{table}

Similarly, Fig.~\ref{fig:machineon2},  confirms that Machine on/off data is completely imbalanced. Out of 1.2 million instances only in 25,000 instances the machine is off. Even though the duration of these stops (downtime) is considerable (Fig.~\ref{fig:detailedanalysis} (a)), still their frequency is only 2\%. Meanwhile, majority of the machine learning algorithms presume that the data samples are divided equally among the classes for that reason the predictions for minority target class ``machine is off" leads to incorrect results.



\begin{figure}
\centering
\includegraphics[width=0.36\textwidth]{machineon} 
\caption{Machine On/Off histogram}
\label{fig:machineon2}
\end{figure}



\subsubsection {Random Data Sampling}
\label{sec:sampling}
As, the minority target class ``machine is off" is the main focus of prediction, thus the data set has to be resampled. Two common approaches are \emph{oversampling} that is to add instances of the minority class ``machine is on" and \emph{undersampling} that is to delete instances of the majority class ``machine is on" \cite{Gonzales}. In this paper, both undersampling and oversampling are used to adjust the class distribution of the sensor data set. Undersampling divides the machine on/off frequency into a 50/50 ratio that means, randomly selecting 25,000 instances where ``machine is on" and 25,000 instances where ``machine is off". Underssampling, dramatically reduces the sample size by discarding useful data, however, the predictions are not biased. On the other hand, oversampling creates synthetic data from the minority class to reach an equal balance between the minority and majority classes. In oversampling, most of the data is retained since no rows are deleted. Oversampling increases the chances of \emph{overfitting} for the reason that it duplicates the existing data. Overfitting occurs when a model can memorize the training data. based on that memorization it makes correct predictions if the test data is exactly similar to the training data, otherwise makes incorrect predictions with unseen test data.

\begin{table}[ht]
\caption{Average scores}
\label{example4}
\centering
\begin{tabular}{ccccc}
\hline\noalign{\smallskip}
 & Undersampled & Undersampled & Oversampled & Oversampled \\
  & (Detailed data) & (Aggregated data) & (Detailed data) & (Aggregated data) \\
  
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Accuracy      &        68\% &68\% &73\% &56\% \\
Precision     &         69\% & 69\% &99\% & 89\%\\
Recall         &        66\% & 67\% &72\% & 56\%  \\
\hline
\end{tabular}
\end{table}

Moreover, logistic regression, support vector machine, k-nearest neighbors and decision tree are used to predict that whether the ``machine is on or off" using undersampled and oversampled data. Table~\ref{example4}, presents the accuracy, \emph{precision} and \emph{recall} scores of these models. Precision means that how valuable the prediction results are and recall means that how comprehensive the prediction results are. The accuracy, precision and recall of the models with undersampled data set is almost same and approximately 68\%. In contrast, the accuracy, precision and recall with the oversampled data set is variable. Oversampled detailed data has a very high precision and high accuracy and recall. Similarly, oversampled aggregated data has high precision with low accuracy and recall.






\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{rocundersample} 
\caption{ROC curves (undersampled)}
\label{fig:machineon}
\end{figure}
















\iffalse
Number of data points in the minority class. Picking the indices of the normal classes. Out of the indices we picked, randomly select "x" number (numberrecordsfraud). Appending the 2 indices. Under sample dataset.
Splitting data into train and test set. Cross validation will be used when calculating accuracies.



\noindent
\begin{verbatim}
1 X = df.loc[:, df.columns != 'MachineOn']
2 y = df.loc[:, df.columns == 'MachineOn']
3 machineoff = len(df[df.MachineOn == 0])
4 machineoff_indices = np.array(df[df.MachineOn == 0].index)
5 machineon_indices = df[df.MachineOn == 1].index
6 random_machineon_indices = np.random.choice(machineOn_indices, 
    number_records_machineOff, replace = False)
7 random_normal_indices = np.array(random_machineOn_indices)
8 under_sample_indices = np.concatenate([machineOff_indices,
    random_machineOn_indices])
9 under_sample_data = df.iloc[under_sample_indices,:]
10 X_undersample = under_sample_data.ix[:, 
    under_sample_data.columns != 'MachineOn']
11 y_undersample = under_sample_data.ix[:, 
    under_sample_data.columns == 'MachineOn']
\end{verbatim}
\noindent






Logistic regression classifier - Undersampled data



\fi




\iffalse

evaluate the model with some performance measure 
consult the domain expert
Takt time is the average time between the start of production of one unit and the start of production of the next unit, when these production starts are set to match the rate of customer demand. For example, if a customer wants 10 units per week, then, given a 40-hour work week and steady flow through the production line, the average time between production starts should be 4 hours (actually less than that in order to account for things like machine downtime and scheduled paid employee breaks), yielding 10 units produced per week.
\section {Near Real-time Dashboard}
\label{sec:dashboard}
\fi
%check if the model assumptions are satisfied
%set up initial parameters
%run a model
%\fi
%\iffalse

\iffalse

nadeem
\fi

\section {Evaluation}
\label{sec:evaluation}
\subsection{Overall Equipment Effectiveness}
\iffalse
This section evaluates the Overall Equipment Effectiveness (OEE)  of Dolle's manufacturing process. OEE calculates the percentage of manufacturing time that is actually productive. It can be used as a benchmark as well as a baseline. It is one of the most widely used standards to calculate productivity in manufacturing industry. In general, OEE consists of three factors, which are \emph{availability}, \emph{performance} and \emph{quality} \cite{oee}. Availability considers all the incidents that stop the planned production. Performance considers those events that causes the manufacturing process to run at less optimal speed. Where as, quality takes into consideration the manufactured products that do not meet the quality standards. An OEE score of 100\% means that the manufacturing is going along at an optimal pace, without any unplanned stops and producing only good quality products. 
\fi
In this section Dolle's manufacturing process is evaluated. This is done by computing the Overall Equipment Effectiveness (OEE) of the manufacturing process. OEE calculates the percentage of manufacturing time that is actually productive. It can be used as a benchmark as well as a baseline. It is one of the most widely used standards to calculate productivity in manufacturing industry. In general, OEE consists of three factors, which are \emph{availability}, \emph{performance} and \emph{quality} \cite{oee}. Availability considers all the incidents that stop the planned production. Performance considers those events that causes the manufacturing process to run at less optimal speed. Where as, quality takes into consideration the part of the manufactured products that do not meet quality standards. An OEE score of 100\% means that the manufacturing is going along at a perfect pace, without any unplanned stops and producing only good quality products. 

\begin{table}[]
\label{tab:distance}
\centering
\caption{Dolle's data for the morning shift}
\begin{tabular}{p{5.7cm}|p{5.7cm}}
%\begin{tabular}{c|c}
\hline
\multicolumn{1}{l}{\textbf{Item}} &  \textbf{Data} \\ \hline
Shift length            & 510 minutes\\
Breaks           &  60 minutes\\
Downtime (planned/unplanned stops)            &   80 minutes\\
Ideal time to produce a single ladder   &   60 seconds\\
Total count  & 260 ladders\\
Reject count & 2 ladders\\

\hline
\end{tabular}
\label{tab:data}
\end{table}

\iffalse
To provide a clear picture of the productivity and the areas for further improvements the OEE calculation of Dolle's manufacturing process is performed based on the items and data mentioned in Table~\ref{tab:data}. In order to calculate OEE, these steps are followed. First, \emph{Planned Production Time (PPT)} and \emph{Run Time (RT)} are calculated. The Planned Production Time is the standard shift time excluding the planned breaks, such as lunch/coffee breaks as well as shift change over time. The Run Time is the actual time of production excluding both the planned and unplanned stops, such as job/product switch over stops, stops caused by faulty string or by screw machine error and so on. Afterwards, \emph{Good Count (GC)} is calculated by rejecting the defected ladders.\\
\fi

Based on the items and data mentioned in Table~\ref{tab:data} the OEE can be computed. The OEE will provide a clear picture of the productivity and possibilities for improvements. In order to calculate OEE, the following steps are performed. First, \emph{Planned Production Time (PPT)} and \emph{Run Time (RT)} are calculated. The Planned Production Time is the standard shift time excluding the planned breaks, such as lunch/coffee breaks as well as shift change over time. The Run Time is the actual time of production excluding both the planned and unplanned stops, such as job/product switch over stops, stops caused by faulty string or by screw machine error and so on. Afterwards, \emph{Good Count (GC)} is calculated by rejecting the defected ladders.\\


\emph{PPT: Shift Length - Breaks = 510 minutes - 60 minutes = 450 minutes}\\

\emph{RT: PPT - Stop Time = 450 minutes - 80 minutes = 370 minutes}\\

\emph{GC: Total Count - Reject Count = 260 ladders - 2 ladder = 258 ladders} \\

\iffalse
Next, \emph{Availability (A)}, \emph{Performance (P)} and \emph{Quality (Q)} are calculated. Availability, calculates the time when the manufacturing process is not running or machine is ``OFF'' for some reasons. It takes into account machine failure (unplanned stops) and setup for next job and/or adjustments (planned stops). Performance, estimates that whether the process is running at its optimal pace and quality concerns with quality standards of the products being produced.\\
\fi

Next, \emph{Availability (A)}, \emph{Performance (P)} and \emph{Quality (Q)} are to be calculated. Availability is the time when the manufacturing process is not running or the machine is ``OFF'' for some reasons. Availability takes machine failure (unplanned stops) and setup for next job and/or adjustments (planned stops) into account. Performance, estimates that whether the process is running at its optimal pace and quality concerns with quality standards of the products being produced.\\


\emph{A: RT / PPT = 370 minutes / 450 minutes = 0.8222 = 82.22\%}\\

\emph{P: (Ideal Production Time * Total Count) / RT} = 

\hspace{12mm} \emph{(60 seconds * 260 ladders)/(370 * 60 seconds) = 0.7027 = 70.27\%}\\

\emph{Q: Good Count / Total count = 258 ladders/ 260 ladders = 0.9923 = 99.23\%}\\

\iffalse
Finally, the OEE score is computed by multiplying the availability, performance and quality. An OEE score of 57.33\% is fairly typical for automate manufacturing industry, however, it indicates there is significant opportunity for improvement in performance. The performance score can be improved by reducing the switch over time between the jobs, by identifying the reasons for machine stops and finally by tackling the major cause(s) of downtime and so on. An OEE score around 85\% is considered a world class for automate manufacturing, where as, an OEE score of 40\% or below is considered low.\\ 

\emph{OEE: A * P * Q = 0.8222 * 0.7027 * 0.9923 = 0.5733 = 57.33\%}\\
\fi

Finally, the OEE score is computed by multiplying the availability, performance and quality.\\ 

\emph{OEE: A * P * Q = 0.8222 * 0.7027 * 0.9923 = 0.5733 = 57.33\%}\\

An OEE score of 57.33\% is fairly typical for automate manufacturing industry, however, it indicates there is significant opportunity for improvement in performance. The performance score can be improved by reducing the switch over time between the jobs, by identifying the reasons for machine stops and finally by tackling the major cause(s) of downtime and so on. An OEE score around 85\% is considered a world class for automate manufacturing, where as, an OEE score of 40\% or below is considered low. For better understanding, OEE data for four sequential weeks is computed. Table~\ref{tab:dist}, shows an average OEE number (61\%) that captures, how well Dolle is doing and the three numbers that summarize the primary types of losses (Availability, Performance, and Quality). To conclude, an average OEE score of 61\% is fairly typical for automate manufacturers, however, it shows that there is considerable scope for improvement, specially in the performance loss factor.



\begin{table}[]
\label{tab:distance}
\centering
\caption{OEE data for four sequential weeks}
\begin{tabular}{p{2.4cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}}
\hline
\multicolumn{1}{l}{\textbf{}} &  \textbf{Week 1} & \textbf{Week 2} & \textbf{Week 3}& \textbf{Week 4} & \textbf{Average} \\ \hline
OEE             & 59\%& 56\% &  63\% & 66\% & 61\%\\
Availability             & 90\%& 89\% &  91\% & 90\% & 90\%\\
Performance            & 66\%& 63\% &  71\% & 75\% & 68.75\% \\
Quality             & 99\%& 98\% &  97\% & 99\% & 98.50\%\\
\hline
\end{tabular}
\label{tab:dist}
\end{table}


\subsection{Performance of Machine Learning Models}

In this article, we show that logistic regression andartificial neural networks share common roots in sta-tistical pattern recognition, and how the latter modelcan be seen as a generalization of the former. 

We brieflycompare these two methods with other popular classification algorithms from the machine learning field, suchask-nearest neighbors, decision trees, and support vec-tor machines.


\iffalse
ogistic re-gression (LR) and artificial neural networks (ANN).These models have their origins in two different com-munities (statistics and computer science), but sharemany similarities. sta-tistical pattern recognition

This section measures the performance in terms of accuracy, precision and recall (Table~\ref{tab:evaluation}) of the chosen classification technique - logistic regression. Accuracy means that ``how often is the classifier correct?" Thus, the proposed algorithm has an accuracy of 80\%, that is, for each 100 ``machineOn = 1" or ``machineOn = 0" types it classified, 80 were correctly classified. Precision means that ``when the model predicts the positive result ``MachineOn = 1", how often is it correct?"
\fi

\begin{table}[]
\label{tab:distance}
\centering
\caption{Model evaluation}
\begin{tabular}{p{2.4cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}p{1.6cm}}
\hline
\multicolumn{1}{l}{\textbf{}} &  \textbf{Scores} \\ \hline
Accuracy      &        80\% \\
Precision     &         95\%\\
Recall         &        64\%\\
\hline
\end{tabular}
\label{tab:evaluation}
\end{table}

\iffalse
 Hence, the algorithm has a precision of 95\%, which means for every 100 ``MachineOn = 1" types it classified, 95 were correctly classified. Recall means that “when it is actually the positive result ``MachineOn = 1", how often does it predict correctly?” As a result, it has a recall of 64\% that means for each 100 ``MachineOn = 1" types it classified in the test data set, 64 were correctly classified.


\fi







\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{detailedddata2} 
\caption{confusion matrices detailed data (undersampling)}
\label{fig:confusion}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{aggregateddata} 
\caption{confusion matrices aggregated data (undersampling)}
\label{fig:confusion}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{confustionmatrixNW} 
\caption{confusion matrices}
\label{fig:confusion}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{accuracyscorelogisticregression} 
\caption{Accuracy scores}
\label{fig:confusion}
\end{figure}

\iffalse
Further, Fig.~\ref{fig:confusion} presents a confusion matrix using heat map. The matrix shows that out of 7660 actual instances (first row) of ``MachineOn = 0" (true negative), the classifier predicted correctly 7400 (96\%) of them. Similarly, out of 8030 instances (second row) of ``MachineOn = 1" (true positive), the classifier predicted correctly 5140 (64\%) of them. In the heat map, diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. Finally, Fig.~\ref{fig:auc} demonstrates a receiver operating characteristics curve (ROC) or area under the curve (AUC), which is commonly used to determine the performance of the classification algorithm. If the AUC score is close to 0.5, the classifier is just doing the random predictions, however, it predicts better as the score approaches close to 1. The AUC score of the proposed classifier is 0.87. It means that the classifier is quite decent at minimizing false negatives (``MachineOn = 1" which is classified as ``MachineOn = 0") and true negatives (''MachineOn = 0" which is classified as ``MachineOn = 0").
\fi




\iffalse





drill down/ drill through analysis
root cause analysis
correlation analysis


present root causes if possible
define some actions based on the given insight
\fi
\iffalse

\subsubsection{Sample Heading (Third Level)} Only two levels of
headings should be numbered. Lower level headings remain unnumbered;
they are formatted as run-in headings.

\paragraph{Sample Heading (Fourth Level)}
The contribution should contain no more than four levels of
headings. Table~\ref{tab1} gives a summary of all heading levels.

\begin{table}
\caption{Table captions should be placed above the
tables.}\label{tab1}
\begin{tabular}{|l|l|l|}
\hline
Heading level &  Example & Font size and style\\
\hline
Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
\hline
\end{tabular}
\end{table}


\noindent Displayed equations are centered and set on a separate
line.
\begin{equation}
x + y = z
\end{equation}
Please try to avoid rasterized images for line-art diagrams and
schemas. Whenever possible, use vector graphics instead (see
Fig.~\ref{fig1}).

\begin{figure}
\includegraphics[width=\textwidth]{fig1.eps}
\caption{A figure caption is always placed below the illustration.
Please note that short captions are centered, while long ones are
justified by the macro package automatically.} \label{fig1}
\end{figure}

\begin{theorem}
This is a sample theorem. The run-in heading is set in bold, while
the following text appears in italics. Definitions, lemmas,
propositions, and corollaries are styled the same way.
\end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
\begin{proof}
Proofs, examples, and remarks have the initial word in italics,
while the following text appears in normal font.
\end{proof}
For citations of references, we prefer the use of square brackets
and consecutive numbers. Citations using labels or the author/year
convention are also acceptable. The following bibliography provides
a sample reference list with entries for journal
articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
and a homepage~\cite{ref_url1}. Multiple citations are grouped
\cite{ref_article1,ref_lncs1,ref_book1},
\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%

\fi

\section{Related Work}
\label{sec:relatedwork}
In general, smart manufacturing covers a number of different technologies, such as, data processing or data analytics, connected devices as well as services, robotics and so on. 

This section mainly concentrates on the previous work done in relation to predictive data analytics for smart manufacturing. According to Lee et al. \cite{Lee}, smart manufacturing still lacks smart analytical techniques and tools. In order to improve productivity, performance of the manufacturing machinery should be measured and optimized with the help of data analytics technologies. A state-of-the-art review of deep learning techniques for machinery fault diagnosis, predictive analytics and defect prognosis is presented by \cite{Wang}. Similarly, big data analytics in semiconductor manufacturing industry was studied by \cite{Moyne}. Further, M\"uller et al. \cite{Muller}, described that big data analytical assets are associated with an average of 3-7 \% improvement in firm productivity. Tao et al. \cite{Tao}, mentioned that data analytics provides an opportunity in the manufacturing industry to adopt data-driven strategies in order to become more competitive. Further, a survey by Kamble et al. \cite{Kamble}, highlighted that the manufacturing industry has realized that the data analytics capabilities are must for future growth. Moreover, Auschitzky et al. \cite{Auschitzky}, proposed the use of advanced analytics such as, data visualization, correlation analysis and artificial neural networks to take a deep dive into historical data, in order to identify initial patterns. Predicting the bottlenecks in a production system based on the active periods of the machines using auto-regressive integrated moving average (ARIMA) method was proposed by \cite{Subramaniyan}. Similarly, a big data analytical architecture for product life cycle management as well as cleaner manufacturing was presented by \cite{Zhang}. Furthermore, Shin et al. \cite{Shin}, presented an analytic model for predicting energy consumption of manufacturing machinery. Chen et al. \cite{Chen}, proposed a Quality of Service (QoS) to manage data traffic using deep learning in the small and medium sized industry. Moreover, Candanedo et al. \cite{Candanedo}, applied machine learning models such as, logistic regression and random forest to predict equipment performance using the historical data set. These works focus on various aspects and recent advancements of data analytics technologies/techniques in smart manufacturing. The work presented in this paper is build on top of the ideas presented in those works. Most of them focus on theoretical rather than practical issues in relation to storage, management, processing and prediction of machine-related data. Hence, the focus of this paper is to provide the practical application of the selected data analysis and machine learning techniques.

As, it can be seen from the above mentioned literature, sensor data analysis in manufacturing industry remains briefly addressed, for that reason this paper is among the very few that provides an in-depth sensor data analysis of imbalanced data using machine learning in order to enhance operational efficiency for small and medium-sized manufacturing industry based on the real world case study.

\section{Conclusions and Future Work}
\label{sec:conclusionandfuturework}
This paper described the data (descriptive statistics analysis) and machine learning techniques built around the concept of industry 4.0. Descriptive statistics helped to describe and understand the features of data such as, correlation, skewness, kurtosis, class distribution and so on. This paper also revealed that descriptive analysis is necessary to build effective machine learning models. Further, various machine learning algorithms such as, logistic regression, neural networks, support vector machines, decision trees and k-nearest neighbors were applied on a historical data set to predict costly production line disruptions. The accuracy of the proposed machine learning models were tested on a real-world data set. The results have validated the effectiveness of the proposed models.



\iffalse





\fi

For the future work, a near real-time anomaly detection mechanism using the machine learning models presented in this paper will be developed that can detect events that do not match to an expected pattern. In addition, a near real-time dashboard will be developed to display the input pace, output pace, screw errors, faculty strings, OEE score and so on.

\iffalse
This paper presents the fundamental concepts of data analytics based on a real world case study. These concepts include data understanding, data preparation,  data pipeline, and big data technologies. To enhance the operational efficiency in-depth descriptive and predictive analysis were performed. Supervised machine learning technique was used to create the classification model to predicts machine stops. In addition, the overall equipment effectiveness (OEE) and the performance of the prediction method were comprehensively evaluated. The OEE results have drawn attention towards improving the production performance by reducing the  machine downtime. Whereas, the predictions made by the model are quite acceptable in terms of predicting the unplanned stops, as unplanned stops are one of the main reasons of reduced production performance. Moreover, to analyse data is important not only for the smart manufacturing business but also for any other type of industry in which significant amounts of sensor/machine data is generated. Hence, the data analytics techniques presented in this paper are general.

For the future work, several prediction based machine learning models will be used and compared. In addition, a near real-time dashboard will be developed to display the input/output pace along with the OEE information. Finally, it will be investigated that how descriptive analysis, predictive analysis and near real-time dashboard help the smart manufacturing companies in general, to enhance their operational efficiency and productivity.
\fi

\section{Acknowledgement}
This  research is supported  by University College of Northern Denmark - Research and Development funding and Dolle A/S.

\begin{thebibliography}{8}

\bibitem{industry}
Luz Mart\'in-Pe\~na, M., D\'iaz-Garrido, E., S\'anchez-L\'opez, J. M.: The Digitalization and Servitization of Manufacturing: A Review on Digital Business Models. Strategic Change, \textbf{27}(2), 91--99 (2018)

\bibitem{CRM}
Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., Wirth, R.:
CRISP-DM 1.0: Step-by-step Data Mining Guide. Technical Report. The CRISP-DM Consortium (2000)

\bibitem{Dolle}
Dolle, \url{http://www.dolle.eu}. Last accessed 11 Nov. 2019





\bibitem {nadeem}
Iftikhar, N., Andersen, T. B., Nordbjerg, F. E., Bobolea, E., Radu, P. B.: Data Analytics for Smart Manufacturing: A Case Study. In 8th International Conference on Data Science, Technology and Applications, pp. 392-399. SCITEPRESS (2019)


\bibitem{oee}
Overall Equipment Effectiveness (OEE), \url{https://www.oee.com}. Last accessed 13 Feb. 2019


%\bibitem {bigdata}
%Ward, J. S., Adam, B.: Undefined by Data: A Survey of Big Data Definitions. arXiv Preprint arXiv:1309.5821 (2013)


\bibitem{iftikhar}
Iftikhar, N., Liu, X., Nordbjerg, F. E.: Relational-Based Sensor Data Cleansing. In East European Conference on Advances in Databases and Information Systems, pp. 108-118. Springer, Cham (2015)

\bibitem{gooijer} De Gooijer, J. G., Hyndman, R. J.:  25 Years of Time Series Forecasting, International Journal of Forecasting, \textbf{22}(3), 443--473 (2006)

\bibitem{Gonzales} 
Gonz\`alez, S., Garc\'ia, S., Li, S. T., Herrera, F.: Chain based Sampling for Monotonic Imbalanced Classification. Information Sciences, 474, 187-204 (2019)

\bibitem{nw}
Dreiseitl, S., Ohno-Machado, L.: Logistic Regression and Artificial Neural Network Classification Models: A Methodology Review, Journal of Biomedical Informatics, \textbf{35}(5-6), 352--359 (2002)

\bibitem{Lee} 
Lee, J., Kao, H.-A., Yang, S.: Service Innovation and Smart Analytics for Industry 
4.0 and Big Data Environment. Procedia CIRP. 16, 3--8 (2014)

\bibitem{Wang} 
Wang, J., Ma, Y., Zhang, L., Gao, R. X., Wu, D.: Deep Learning for Smart Manufacturing: Methods and Applications. Journal of Manufacturing Systems (2018)

\bibitem{Moyne}
Moyne, J., Iskandar, J.: Big Data Analytics for Smart Manufacturing: Case Studies in Semiconductor Manufacturing. Processes \textbf{5}(3), 39--58 (2017)

\bibitem{Muller}
M\"uller, O., Fay, M., vom Brocke, J.: The Effect of Big Data and Analytics on Firm Performance: An Econometric Analysis Considering Industry Characteristics. Journal of Management Information Systems \textbf{35}(2), 488--50 (2018)

\bibitem{Tao}
Tao, F., Qi, Q., Liu, A., Kusiak, A.: Data-driven Smart Manufacturing. Journal of Manufacturing Systems (2018)

\bibitem{Kamble}
Kamble, S.S., Gunasekaran, A., Gawankar, S.A.: Sustainable Industry 4.0 Framework: A Systematic Literature Review Identifying the Current Trends and Future Perspectives. Process Safety and Environmental Protection \textbf{117}, 408--425 (2018) 

\bibitem{Auschitzky}
Auschitzky, E, Markus, H., Agesan R.: How Big Data can Improve Manufacturing. McKinsey \& Company 822 (2014)

\bibitem{Subramaniyan}
Subramaniyan, M., Skoogh, A., Salomonsson, H., Bangalore, P., Bokrantz, J.: A Data-Driven Algorithm to Predict throughput Bottlenecks in a Production System based on Active Periods of the Machines. Computers \& Industrial Engineering (2018)

\bibitem{Zhang}
Zhang, Y., Ren, S., Liu, Y., Si, S.: A Big Data Analytics Architecture for Cleaner Manufacturing and Maintenance Processes of Complex Products. Journal of Cleaner Production \textbf{142}, 626--641 (2017) 

\bibitem{Shin}
Shin, S. J., Woo, J., Rachuri, S.: Predictive Analytics Model for Power Consumption in Manufacturing. Procedia CIRP \textbf{15}, 153--158 (2014)

\bibitem{Chen} Chen, Z., Luo, L., Yang, H., Yu, J., Wen, M., Zhang, C.:  GENIE: QoS-guided Dynamic Scheduling for CNN-based Tasks on SME Clusters. In Design, Automation and Test in Europe Conference and Exhibition, pp. 1599-1602. IEEE (2019)

\bibitem{Candanedo} Candanedo, I.S., Nieves, E.H., González, S.R., Martín, M.T.S., Briones, A.G.: Machine Learning Predictive Model for Industry 4.0. In International Conference on Knowledge Management in Organizations pp. 501-510. Springer, Cham (2018)

%\bibitem{ref_article1}
%Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

%\bibitem{ref_lncs1}
%Author, F., Author, S.: Title of a proceedings paper. In: Editor,
%F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
%Springer, Heidelberg (2016). \doi{10.10007/1234567890}

%\bibitem{ref_book1}
%Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
%Location (1999)

%\bibitem{ref_proc1}
%Author, A.-B.: Contribution title. In: 9th International Proceedings
%on Proceedings, pp. 1--2. Publisher, Location (2010)

%\bibitem{ref_url1}
%LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
%Oct 2017
\end{thebibliography}
\end{document}
